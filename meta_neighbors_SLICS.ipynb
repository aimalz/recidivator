{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flexible environment curve generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy as ap\n",
    "from astropy import coordinates\n",
    "from astropy.io import fits\n",
    "import bisect\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess GAMA-like SLICS mock positions and redshifts\n",
    "\n",
    "picking one LOS for now, can rerun for others or convert to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOS = 42\n",
    "loskey = str(LOS)\n",
    "field = loskey\n",
    "key = loskey\n",
    "\n",
    "toppath = '/media/CRP6/Cosmology/recidivator/SLICS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter for data quality -- only needs to be done once, but it's fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# frompath = '/media/CRP6/Cosmology/data/SLICS_GAMA/'\n",
    "\n",
    "# with fits.open(frompath+'FullGalCatalogGAMA_Lcorr_clean_LOS'+loskey+'.fits') as data:\n",
    "# #     print(data[1].header)\n",
    "#     df = pd.DataFrame(data[1].data)\n",
    "    \n",
    "# # plt.scatter(df['x_arcmin_clustering'], df['y_arcmin_clustering'], s=1, alpha=0.1)\n",
    "# # plt.xlim(0, 600)\n",
    "# # plt.ylim(0, 600)\n",
    "\n",
    "# high_q = pd.DataFrame()\n",
    "# high_q['RA'] = df['x_arcmin_clustering'] / 60.\n",
    "# high_q['DEC'] = df['y_arcmin_clustering'] / 60.\n",
    "# high_q['Z'] = df['z_spectroscopic']\n",
    "# high_q.index.names = ['CATAID']\n",
    "\n",
    "# # plt.scatter(high_q['RA'], high_q['DEC'], s=1, alpha=0.1)\n",
    "# # plt.xlim(0, 10)\n",
    "# # plt.ylim(0, 10)\n",
    "\n",
    "# gf = {}\n",
    "# gf[key] = high_q\n",
    "# # # gf['G02'] = high_q.loc[(high_q['RA'] > 30.2) & (high_q['RA'] < 38.8) & (high_q['DEC'] > -10.5) & (high_q['DEC'] < -3.72)]\n",
    "# # # gf['G09'] = high_q.loc[(high_q['RA'] > 129.0) & (high_q['RA'] < 141.0) & (high_q['DEC'] > -2.) & (high_q['DEC'] < 3.)]\n",
    "# # # gf[key] = high_q.loc[(high_q['RA'] > minx) & (high_q['RA'] < maxx) & (high_q['DEC'] > miny) & (high_q['DEC'] < maxy)]\n",
    "# # # gf['G15'] = high_q.loc[(high_q['RA'] > 211.5) & (high_q['RA'] < 223.5) & (high_q['DEC'] > -2.) & (high_q['DEC'] < 3.)]\n",
    "\n",
    "# gf[key].to_csv(toppath+'flexible_envirocurves/LOS'+loskey+'inputs.csv')\n",
    "\n",
    "# # # plt.scatter(gf[key]['RA'], gf[key]['DEC'], s=1)\n",
    "# # # plt.xlim(minx, maxx)\n",
    "# # # plt.ylim(miny, maxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunk up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxang = ap.units.Quantity(2.5, 'deg')\n",
    "\n",
    "#max number of galaxies per chunked file\n",
    "maxfile = 2**14\n",
    "share_index = range(maxfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, chunk up big LOS file, which is fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old way to get neighbors on real data\n",
    "\n",
    "# def get_neighbors(keyno):\n",
    "#     field = list(gf.keys())[keyno]\n",
    "#     small_piece = pd.read_csv(toppath+'flexible_envirocurves/LOS'+field+'inputs.csv', index_col=['CATAID'])\n",
    "# #     print(small_piece.iloc[99])\n",
    "#     coords = ap.coordinates.SkyCoord(small_piece['RA'], small_piece['DEC'], unit='deg')\n",
    "#     #warning: slow!!! (not actually that slow)\n",
    "#     output = ap.coordinates.search_around_sky(coords, coords, maxang)\n",
    "#     pkl.dump(output, open(toppath+'flexible_envirocurves/neighbors_'+field+'_allz.pkl', 'wb'))\n",
    "#     return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_down_neighbor_input(keyno):\n",
    "    big_in = pd.read_csv(toppath+'flexible_envirocurves/LOS'+field+'inputs.csv', index_col=['CATAID'])\n",
    "    big_out = big_in.copy()\n",
    "    nbig = len(big_in)# total number of galaxies in big file\n",
    "    nfile = 0\n",
    "    paircount = []\n",
    "    while nfile * maxfile <= nbig:\n",
    "        glob_ind_min = nfile * maxfile# min real index for this chunk\n",
    "        glob_ind_max = (nfile + 1) * maxfile\n",
    "        if nbig < glob_ind_max:\n",
    "            chunk_ind_max = nbig - glob_ind_min\n",
    "            glob_ind_max = nbig\n",
    "        else:\n",
    "            chunk_ind_max = maxfile\n",
    "        paircount.append((glob_ind_min, glob_ind_max))\n",
    "        small_piece = big_in[glob_ind_min : glob_ind_max]\n",
    "        big_out['chunk_'+str(nfile)] = np.nan\n",
    "        big_out['chunk_'+str(nfile)][glob_ind_min : glob_ind_max] = share_index[:chunk_ind_max]\n",
    "        small_piece.to_csv(toppath+'flexible_envirocurves/LOS'+field+'chunk'+str(nfile)+'.csv')\n",
    "        print('separated out chunk '+str(nfile)+' of LOS '+field+': '+str(paircount[-1]))\n",
    "        nfile += 1\n",
    "#     paircount = nfile\n",
    "    big_out.to_csv(toppath+'flexible_envirocurves/LOS'+field+'allchunks.csv')\n",
    "    minx, maxx = min(big_in['RA']), max(big_in['RA'])\n",
    "    miny, maxy = min(big_in['DEC']), max(big_in['DEC'])\n",
    "    extrema = (minx, maxx, miny, maxy)\n",
    "    return nfile, paircount, extrema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nps = 1#len(gf.keys()) #mp.cpu_count()\n",
    "# pool = mp.Pool(nps)\n",
    "# paircounts = pool.map(break_down_neighbor_input, range(1))#range(len(gf.keys())))\n",
    "# # print(paircounts)\n",
    "# with open(toppath+'flexible_envirocurves/LOS'+field+'chunkinfo.csv', 'wb') as chunkinfo:\n",
    "#     pkl.dump(paircounts, chunkinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set bounds for normalizing neighbors per area and magic numbers for manipulating chunked files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(toppath+'flexible_envirocurves/LOS'+field+'chunkinfo.csv', 'rb') as chunkinfo:\n",
    "    paircounts = pkl.load(chunkinfo)\n",
    "\n",
    "nfile = paircounts[0][0]\n",
    "indends = paircounts[0][1]\n",
    "(minx, maxx, miny, maxy) = paircounts[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find neighbors with astropy\n",
    "\n",
    "must be parallelized because still pretty slow, but should not need to run more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_pairs = {}\n",
    "pairkey = 0\n",
    "for i in range(nfile):\n",
    "    for j in range(nfile):\n",
    "        chunk_pairs[pairkey] = (i, j)\n",
    "        pairkey += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors_breakup(keyno):\n",
    "    field = loskey\n",
    "    (a, b) = chunk_pairs[keyno]\n",
    "    small_a = pd.read_csv(toppath+'flexible_envirocurves/LOS'+field+'chunk'+str(a)+'.csv', index_col=['CATAID'])\n",
    "    small_b = pd.read_csv(toppath+'flexible_envirocurves/LOS'+field+'chunk'+str(b)+'.csv', index_col=['CATAID'])\n",
    "#     big_piece = pd.read_csv(toppath+'flexible_envirocurves/LOS'+field+'inputs.csv', index_col=['CATAID'])\n",
    "#     small_piece = break_down_neighbor_input(big_piece)\n",
    "#     print(small_piece.iloc[99])\n",
    "    coords_a = ap.coordinates.SkyCoord(small_a['RA'], small_a['DEC'], unit='deg')\n",
    "    coords_b = ap.coordinates.SkyCoord(small_b['RA'], small_b['DEC'], unit='deg')\n",
    "    #warning: slow!!! (not actually that slow)\n",
    "    print('started finding neighbors')\n",
    "    output = ap.coordinates.search_around_sky(coords_a, coords_b, maxang)\n",
    "    print('finished finding neighbors')\n",
    "    pkl.dump(output, open(toppath+'flexible_envirocurves/neighbors_LOS'+field+'chunk'+str(a)+'Xchunk'+str(b)+'allz.pkl', 'wb'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nps = 35#pairkey #mp.cpu_count()\n",
    "# pool = mp.Pool(nps)\n",
    "# pool.map(get_neighbors_breakup, range(pairkey))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse the neighbor info to make curves\n",
    "\n",
    "have to do this for every set of distances because too memory intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with many more choices for this!\n",
    "ndist = 100# 128\n",
    "# distance_evaluation_points = np.exp(np.linspace(np.log(2.5 / 60. / 60.), np.log(2.5), ndist+2)[1:-1])\n",
    "distance_evaluation_points = np.linspace(0., maxang.value, ndist+2)[1:-1]\n",
    "# pos = bisect.bisect(distance_evaluation_points, maxang.value)\n",
    "# distance_evaluation_points = distance_evaluation_points[:pos]\n",
    "# n_dists = len(distance_evaluation_points)\n",
    "# iter_over_dists = enumerate(distance_evaluation_points)\n",
    "# ndist = range(len(distance_evaluation_points))\n",
    "# with open(toppath+'flexible_envirocurves/eval'+str(ndist)+'dists_LOS'+field+'.pkl', 'wb') as savedists:\n",
    "#     pkl.dump(distance_evaluation_points, savedists)\n",
    "print(distance_evaluation_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(r, d, theta=None):\n",
    "    if theta == None:\n",
    "        theta = 2. * np.arccos(d / r)\n",
    "    return r**2 * (theta - np.sin(theta)) / 2.\n",
    "\n",
    "def sector(r, d, theta=None):\n",
    "    if theta == None:\n",
    "        theta = np.arcsin(d / r)\n",
    "    return r**2 * theta / 2.\n",
    "\n",
    "# this throws an error at the points used to define minx, maxx, miny, maxy\n",
    "def area(r, x, y, minx, maxx, miny, maxy, vb=True):\n",
    "    lx = x - minx\n",
    "    ux = maxx - x\n",
    "    ly = y - miny\n",
    "    uy = maxy - y\n",
    "    distances = np.array([lx, ux, ly, uy])\n",
    "    #print(distances)\n",
    "    condition = (distances >= r)\n",
    "    ntrue = sum(condition)\n",
    "    if ntrue == 4:\n",
    "        return np.pi * r**2\n",
    "    elif ntrue == 3:\n",
    "        return np.pi * r**2 - segment(r, min(distances))\n",
    "    elif ntrue == 2:\n",
    "        if vb: print('radii should be chosen so that these cannot be parallel, \\\n",
    "                but will at some point add in a check for this')\n",
    "        distx = min(distances[:2])\n",
    "        disty = min(distances[-2:])\n",
    "        if np.sqrt(distx**2 + disty**2) < r:\n",
    "            thetax = np.arcsin(distx / r)\n",
    "            thetay = np.arcsin(disty / r)\n",
    "            areax = distx * r * np.cos(thetax) / 2.\n",
    "            areay = disty * r * np.cos(thetay) / 2.\n",
    "            return sector(r, distx, theta=thetax) + sector(r, disty, theta=thetay) + \\\n",
    "                            sector(r, r, theta=np.pi / 2.) + distx * disty + areax + areay\n",
    "        else:\n",
    "            return np.pi * r**2 - segment(r, distx) - segment(r, disty)\n",
    "    else:\n",
    "        if vb: print('this case should not happen because we did not consider radii \\\n",
    "                beyond half the shortest side of the footprint,\\\n",
    "                but will at some point deal with this case')\n",
    "        return None\n",
    "\n",
    "\n",
    "# ## Calculates volume normalized environment\n",
    "# def calc_env(ind):\n",
    "#     \"\"\"\n",
    "#         Runs galenv to calculate galaxy environment.\n",
    "#         This is set up to run in the multiprocessing so a lot of inputs are\n",
    "#         not set when you call the function, but are supposed to be defined\n",
    "#         when running this code.\n",
    "\n",
    "#         Output: nearest neighbors at a given angular separation.\n",
    "#     \"\"\"\n",
    "#     if opts.run_environment:\n",
    "#         # Generates environments for GAMA RA/Dec data\n",
    "#         res = [subsamples[f][s]['CATAID'].values[ind]]\n",
    "#     if opts.run_particle_environment:\n",
    "#         # Generates environments for particle RA/Dec data\n",
    "#         res = [ind]\n",
    "\n",
    "#     friends = data\n",
    "#     for dist in try_distances:\n",
    "#         friends = galenv.nn_finder(friends, data[ind], dist)\n",
    "#         #print('r/dist', dist, 'x ', data[ind][0], 'y ', data[ind][1], 'minx', minx, 'maxx', maxx, 'miny', miny, 'maxy', maxy)\n",
    "#         vol = area(dist, data[ind][0], data[ind][1], minx, maxx, miny, maxy, vb=False)\n",
    "#         #print('vol', vol)\n",
    "#         res.append(float(len(friends)) / vol)\n",
    "#     return res\n",
    "# ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(nfile):\n",
    "#     with open(toppath+'flexible_envirocurves/LOS'+loskey+'chunk'+str(i)+'.csv', 'rb') as smallfile:\n",
    "    small_piece = pd.read_csv(toppath+'flexible_envirocurves/LOS'+loskey+'chunk'+str(i)+'.csv', index_col='CATAID')\n",
    "#     small_piece['neighbor_CATAID'] = small_piece.index\n",
    "#     small_piece['neighbor_CATAID'] = small_piece.apply(lambda row: [row['neighbor_CATAID']], axis=1)\n",
    "#     small_piece['neighbor_dist'] = small_piece.apply(lambda row: [0.], axis=1)\n",
    "#     with open(toppath+'flexible_envirocurves/eval'+str(ndist)+'dists_LOS'+field+'.pkl', 'rb') as savedists:\n",
    "    which_dists = distance_evaluation_points\n",
    "#     for dist in which_dists:\n",
    "#         small_piece[str(dist)] = 0\n",
    "    for k, dist in enumerate(which_dists):\n",
    "        small_piece[str(dist)] = np.zeros_like(len(small_piece))\n",
    "    for j in range(nfile):\n",
    "        with open(toppath+'flexible_envirocurves/neighbors_LOS'+loskey+'chunk'+str(i)+'Xchunk'+str(j)\n",
    "                  +'allz.pkl', 'rb') as tomerge:\n",
    "            all_pairs = pkl.load(tomerge)\n",
    "        inds_with_neighbors, where_my_neighbors_at = np.unique(all_pairs[0], return_inverse=True)\n",
    "#         for ind in np.unique(all_pairs[0])[:10]:\n",
    "        def help_gather_neighbors(ind):\n",
    "            newrow = np.zeros_like(which_dists)\n",
    "            if ind in inds_with_neighbors:\n",
    "#                 ra = small_piece.iloc[ind + indends[i][0]]['RA']\n",
    "#                 dec = small_piece.iloc[ind + indends[i][0]]['DEC']\n",
    "#             where_my_neighbors_at = np.where(all_pairs[0] == ind)\n",
    "#             my_neighbor_inds = all_pairs[1][where_my_neighbors_at == ind]\n",
    "#             tosave = list(my_neighbor_inds + indends[j][0])\n",
    "#             print(tosave)\n",
    "#             small_piece['neighbor_CATAID'].loc[ind + indends[i][0]] += list(my_neighbor_inds + indends[j][0])\n",
    "#             where_to_save = ind + indends[i][0]\n",
    "#             CATAIDs_to_save = list(my_neighbor_inds + indends[j][0])\n",
    "                my_neighbor_dists = all_pairs[2][where_my_neighbors_at == ind].value\n",
    "#             small_piece['neighbor_dist'].loc[ind + indends[i][0]] += list(my_neighbor_dists)\n",
    "                sort_dists = sorted(list(my_neighbor_dists))\n",
    "                for k, dist in enumerate(which_dists):\n",
    "                    newrow[k] = bisect.bisect(sort_dists, dist)\n",
    "#                     vol = area(dist, ra, dec, minx, maxx, miny, maxy, vb=False)\n",
    "#                     newrow[k] = float(pos) / vol\n",
    "#             out_info = [ind + indends[i][0], CATAIDs_to_save, dists_to_save]\n",
    "#             return(sorted(dists_to_save))\n",
    "            return(newrow)\n",
    "        nps = 35#mp.cpu_count()\n",
    "        pool = mp.Pool(nps)\n",
    "        distcounts = pool.map(help_gather_neighbors, range(len(small_piece.index)))\n",
    "        distcounts = np.array(distcounts).T\n",
    "        print(str((i, j, np.shape(distcounts))))\n",
    "        for k, dist in enumerate(which_dists):\n",
    "            small_piece[str(dist)] += distcounts[k]\n",
    "#         for dist in which_dists:\n",
    "#             small_piece[str(dist)] += bisect.bisect(sort_dists, dist)#.loc[ind + indends[i][0]]\n",
    "#         with open(toppath+'flexible_envirocurves/tmp_LOS'+loskey+'chunk'+str(i)+'Xchunk'+str(j)+'.pkl', 'wb') as tmpfile:\n",
    "#             pkl.dump(ids_and_dists, tmpfile)\n",
    "#     small_piece.apply(lambda row: row['neighbor_CATAID'].pop(0), axis=1)\n",
    "#     small_piece.apply(lambda row: row['neighbor_dist'].pop(0), axis=1)\n",
    "#     print(small_piece)\n",
    "    for k, dist in enumerate(which_dists):\n",
    "        small_piece[str(dist)] = small_piece.apply(lambda row: float(row[str(dist)]) / \n",
    "                                                        area(dist, row['RA'], row['DEC'], \n",
    "                                                             minx, maxx, miny, maxy, vb=False), axis=1)\n",
    "    print(small_piece)\n",
    "#     with open(toppath+'flexible_envirocurves/LOS'+loskey+'chunk'+str(i)+'dists'+str(ndist)+'.csv', 'wb') as smallfile:\n",
    "    small_piece.to_csv(toppath+'flexible_envirocurves/LOS'+loskey+'chunk'+str(i)+'dists'+str(ndist)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # with open(toppath+'flexible_envirocurves/LOS'+loskey+'chunk'+str(0)+'dists'+str(ndist)+'.csv', 'rb') as expfile:\n",
    "# small_piece = pd.read_csv(toppath+'flexible_envirocurves/LOS'+loskey+'chunk'+str(1)+'dists'+str(ndist)+'.csv', index_col='CATAID')\n",
    "# print(small_piece)\n",
    "\n",
    "# # small_piece[str(distance_evaluation_points[0])] += np.ones(len(small_piece.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(toppath+'flexible_envirocurves/neighbors_LOS'+loskey+'chunk'+str(1)+'Xchunk'+str(3)+'allz.pkl', 'rb') as tomerge:\n",
    "    all_pairs = pkl.load(tomerge)\n",
    "    inds_with_neighbors, where_my_neighbors_at = np.unique(all_pairs[0], return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(where_my_neighbors_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inds_with_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_pairs[1][where_my_neighbors_at == 16382])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(toppath+'flexible_envirocurves/LOS'+loskey+'chunk'+str(0)+'.csv', index_col='CATAID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def gather_neighbors(chunkind):\n",
    "#     small_piece = pd.read_csv(toppath+'flexible_envirocurves/LOS'+loskey+'chunk'+str(chunkind)+'.csv')\n",
    "#     small_piece['neighbor_CATAID'] = None\n",
    "#     small_piece['neighbor_dist'] = None\n",
    "#     for i in range(nfile):\n",
    "        \n",
    "#         where_my_neighbors_at = np.where(all_pairs[0] == ind)\n",
    "#         my_neighbor_inds = all_pairs[1][where_my_neighbors_at]\n",
    "#         save_df = df[['CATAID', 'Z']].iloc[my_neighbor_inds]\n",
    "#     #     my_neighbor_zs = df.iloc[my_neighbor_inds]['Z']\n",
    "#         neighbor_dists = all_pairs[2][where_my_neighbors_at]\n",
    "#         save_df['dist'+str(ind)] = neighbor_dists\n",
    "#     #     nn = len(neighbor_distances)\n",
    "#     #     res[ind] = sorted(neighbor_distances)\n",
    "#         output = (ind, df.iloc[ind]['CATAID'], save_df)\n",
    "#     #     res[ind] = output\n",
    "#         return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1):#nfile):\n",
    "#     all_pairs = pd.read_csv(toppath+'flexible_envirocurves/LOS'+loskey+'chunk'+str(i)+'.csv', index_col=['CATAID'])\n",
    "#     def help_find_neighbors(ind):\n",
    "#         where_my_neighbors_at = np.where(all_pairs[0] == ind)\n",
    "#         my_neighbor_inds = all_pairs[1][where_my_neighbors_at]\n",
    "#         save_df = df[['CATAID', 'Z']].iloc[my_neighbor_inds]\n",
    "#     #     my_neighbor_zs = df.iloc[my_neighbor_inds]['Z']\n",
    "#         neighbor_dists = all_pairs[2][where_my_neighbors_at]\n",
    "#         save_df['dist'+str(ind)] = neighbor_dists\n",
    "#     #     nn = len(neighbor_distances)\n",
    "#     #     res[ind] = sorted(neighbor_distances)\n",
    "#         output = (ind, df.iloc[ind]['CATAID'], save_df)\n",
    "#     #     res[ind] = output\n",
    "#         return(output)\n",
    "    \n",
    "#     for j in range(nfile):\n",
    "#         thing = pkl.load(open(toppath+'flexible_envirocurves/neighbors_LOS'+loskey+'chunk'+str(i)+'Xchunk'+str(j)+'allz.pkl', 'rb'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pairs = pkl.load(open(toppath+'flexible_envirocurves/neighbors_'+test_key+'_allz.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add in redshift filtering here or earlier?\n",
    "\n",
    "# def help_find_neighbors(ind):\n",
    "#     where_my_neighbors_at = np.where(all_pairs[0] == ind)\n",
    "#     my_neighbor_inds = all_pairs[1][where_my_neighbors_at]\n",
    "#     save_df = df[['CATAID', 'Z']].iloc[my_neighbor_inds]\n",
    "# #     my_neighbor_zs = df.iloc[my_neighbor_inds]['Z']\n",
    "#     neighbor_dists = all_pairs[2][where_my_neighbors_at]\n",
    "#     save_df['dist'+str(ind)] = neighbor_dists\n",
    "# #     nn = len(neighbor_distances)\n",
    "# #     res[ind] = sorted(neighbor_distances)\n",
    "#     output = (ind, df.iloc[ind]['CATAID'], save_df)\n",
    "# #     res[ind] = output\n",
    "#     return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remember to do for all LOS\n",
    "\n",
    "# # res = [[]] * len(df.index)\n",
    "# nps = 20#mp.cpu_count() - 1\n",
    "# pool = mp.Pool(nps)\n",
    "# try_neighbor_dists = pool.map(help_find_neighbors, df.index)\n",
    "# pkl.dump(try_neighbor_dists, open(toppath+'flexible_envirocurves/parsed_neighbor_dists_'+test_key+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate the curves at angular distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_neighborhood = pkl.load(open(toppath+'flexible_envirocurves/parsed_neighbor_dists_'+test_key+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "the_neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_redshift_neighbors(ind, delta=0.1):\n",
    "    my_neighborhood = the_neighborhood[ind][-1]\n",
    "    all_neighbor_zs = my_neighborhood['Z']\n",
    "    my_z = all_neighbor_zs[ind]\n",
    "    close_neighbor_dists = my_neighborhood['dist'+str(ind)][(all_neighbor_zs > my_z-delta) & (all_neighbor_zs < my_z+delta)]\n",
    "    return(close_neighbor_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in volume normalization here\n",
    "\n",
    "def help_make_curve(ind):\n",
    "    curve = [the_neighborhood[ind][1]]#np.empty((ndist))\n",
    "    (ra, dec) = (df.iloc[ind]['RA'], df.iloc[ind]['DEC'])\n",
    "#     redshifts = test_data[ind][3]\n",
    "    distances = sorted(list(give_redshift_neighbors(ind)))\n",
    "    for dist in distance_evaluation_points:\n",
    "#         print(i)\n",
    "        pos = bisect.bisect(distances, dist)\n",
    "#         print(pos)\n",
    "#         curve[i] = pos\n",
    "#         all_curves.iloc[ind][str(i)] = pos\n",
    "        vol = area(dist, ra, dec, minx, maxx, miny, maxy, vb=False)\n",
    "        curve.append(float(pos) / vol)\n",
    "        \n",
    "    return(curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nps = 10#mp.cpu_count() - 1\n",
    "pool = mp.Pool(nps)\n",
    "try_neighbor_curves = pool.map(help_make_curve, df.index)\n",
    "newdist = np.hstack((np.ones((1)), distance_evaluation_points))\n",
    "save_curves = np.vstack((newdist, try_neighbor_curves))\n",
    "pkl.dump(np.array(save_curves), open(toppath+'flexible_envirocurves/zslice_normed_100dist_'+test_key+'.pkl', 'wb'))\n",
    "pkl.dump(distance_evaluation_points, open(toppath+'flexible_envirocurves/zslice_normed_100dist_'+test_key+'_evaluated_distances.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_q = pd.read_csv(toppath+'test/one_slice/SpecObjPhot.csv', index_col=['CATAID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_q = all_q[all_q['NQ'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_q = high_q.rename(columns={\"Unnamed: 0\": \"SpecObjPhot_index\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split by field for speed, omit field with nonuniform coverage\n",
    "\n",
    "sky coordinate limits came from [GAMA website](https://www.astro.ljmu.ac.uk/~ikb/research/gama_fields/) -- must include DEC for this to work!!!\n",
    "\n",
    "NOTE: this fails on G15 field because it's got too many galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = {}\n",
    "# gf['G02'] = high_q.loc[(high_q['RA'] > 30.2) & (high_q['RA'] < 38.8) & (high_q['DEC'] > -10.5) & (high_q['DEC'] < -3.72)]\n",
    "# gf['G09'] = high_q.loc[(high_q['RA'] > 129.0) & (high_q['RA'] < 141.0) & (high_q['DEC'] > -2.) & (high_q['DEC'] < 3.)]\n",
    "gf['G12'] = high_q.loc[(high_q['RA'] > 174.0) & (high_q['RA'] < 186.0) & (high_q['DEC'] > -3.) & (high_q['DEC'] < 2.)]\n",
    "# gf['G15'] = high_q.loc[(high_q['RA'] > 211.5) & (high_q['RA'] < 223.5) & (high_q['DEC'] > -2.) & (high_q['DEC'] < 3.)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the neighbors within max angular distance set by GAMA footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gal_kept = {}\n",
    "for key in gf.keys():\n",
    "    n_gal_kept[key] = len(gf[key])\n",
    "    gf[key].to_csv(toppath+'flexible_envirocurves/field'+key+'high_q.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or key in gf.keys():\n",
    "    \n",
    "#     n_gal_kept[key] = len(gf[key])\n",
    "#     gf[key].to_csv('flexible_envirocurves/field'+key+'high_q.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_inds = {}\n",
    "for key in gf.keys():\n",
    "    easy_inds[key] = pd.read_csv(toppath+'flexible_envirocurves/field'+key+'high_q.csv')\n",
    "    easy_inds[key].index.rename('field_index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxang = ap.units.Quantity(2.5, 'deg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the slow step!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(keyno):\n",
    "    field = list(gf.keys())[keyno]\n",
    "    small_piece = pd.read_csv(toppath+'flexible_envirocurves/field'+field+'high_q.csv')\n",
    "#     print(small_piece.iloc[99])\n",
    "    coords = ap.coordinates.SkyCoord(small_piece['RA'], small_piece['DEC'], unit='deg')\n",
    "    #warning: slow!!! (not actually that slow)\n",
    "    output = ap.coordinates.search_around_sky(coords, coords, maxang)\n",
    "    pkl.dump(output, open(toppath+'flexible_envirocurves/neighbors_'+field+'_allz.pkl', 'wb'))\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#still kinda slow\n",
    "nps = len(gf.keys()) #mp.cpu_count()\n",
    "pool = mp.Pool(nps)\n",
    "pool.map(get_neighbors, range(len(gf.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(ap.coordinates.search_around_sky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse the neighbor info to flexibly make curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_key = 'G12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(toppath+'flexible_envirocurves/field'+test_key+'high_q.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = pkl.load(open(toppath+'flexible_envirocurves/neighbors_'+test_key+'_allz.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in redshift filtering here or earlier?\n",
    "\n",
    "def help_find_neighbors(ind):\n",
    "    where_my_neighbors_at = np.where(all_pairs[0] == ind)\n",
    "    my_neighbor_inds = all_pairs[1][where_my_neighbors_at]\n",
    "    save_df = df[['CATAID', 'Z']].iloc[my_neighbor_inds]\n",
    "#     my_neighbor_zs = df.iloc[my_neighbor_inds]['Z']\n",
    "    neighbor_dists = all_pairs[2][where_my_neighbors_at]\n",
    "    save_df['dist'+str(ind)] = neighbor_dists\n",
    "#     nn = len(neighbor_distances)\n",
    "#     res[ind] = sorted(neighbor_distances)\n",
    "    output = (ind, df.iloc[ind]['CATAID'], save_df)\n",
    "#     res[ind] = output\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember to do for all fields\n",
    "\n",
    "# res = [[]] * len(df.index)\n",
    "nps = 35#mp.cpu_count() - 1\n",
    "pool = mp.Pool(nps)\n",
    "try_neighbor_dists = pool.map(help_find_neighbors, df.index)\n",
    "pkl.dump(try_neighbor_dists, open(toppath+'flexible_envirocurves/parsed_neighbor_dists_'+test_key+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate the curves at angular distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_neighborhood = pkl.load(open(toppath+'flexible_envirocurves/parsed_neighbor_dists_'+test_key+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "the_neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with many more choices for this!\n",
    "ndist = 100\n",
    "distance_evaluation_points = np.exp(np.linspace(np.log(maxang.value / 60. / 60.), np.log(maxang.value), ndist+2)[1:-1])\n",
    "# n_dists = len(distance_evaluation_points)\n",
    "# iter_over_dists = enumerate(distance_evaluation_points)\n",
    "# ndist = range(len(distance_evaluation_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_evaluation_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xpl = np.linspace(0., 2.5, ndist+2)[1:-1]\n",
    "# plt.scatter(xpl, np.linspace(0., 2.5, ndist+2)[1:-1])\n",
    "# plt.scatter(xpl, np.log(np.logspace(0., np.log(2.5), ndist+2)[1:-1]))\n",
    "# plt.scatter(xpl, np.exp(np.linspace(np.log(2.5 / 60. / 60.), np.log(2.5), ndist+2)[1:-1]))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize the curves by neighbors-per-area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(r, d, theta=None):\n",
    "    if theta == None:\n",
    "        theta = 2. * np.arccos(d / r)\n",
    "    return r**2 * (theta - np.sin(theta)) / 2.\n",
    "\n",
    "def sector(r, d, theta=None):\n",
    "    if theta == None:\n",
    "        theta = np.arcsin(d / r)\n",
    "    return r**2 * theta / 2.\n",
    "\n",
    "# this throws an error at the points used to define minx, maxx, miny, maxy\n",
    "def area(r, x, y, minx, maxx, miny, maxy, vb=True):\n",
    "    lx = x - minx\n",
    "    ux = maxx - x\n",
    "    ly = y - miny\n",
    "    uy = maxy - y\n",
    "    distances = np.array([lx, ux, ly, uy])\n",
    "    #print(distances)\n",
    "    condition = (distances >= r)\n",
    "    ntrue = sum(condition)\n",
    "    if ntrue == 4:\n",
    "        return np.pi * r**2\n",
    "    elif ntrue == 3:\n",
    "        return np.pi * r**2 - segment(r, min(distances))\n",
    "    elif ntrue == 2:\n",
    "        if vb: print('radii should be chosen so that these cannot be parallel, \\\n",
    "                but will at some point add in a check for this')\n",
    "        distx = min(distances[:2])\n",
    "        disty = min(distances[-2:])\n",
    "        if np.sqrt(distx**2 + disty**2) < r:\n",
    "            thetax = np.arcsin(distx / r)\n",
    "            thetay = np.arcsin(disty / r)\n",
    "            areax = distx * r * np.cos(thetax) / 2.\n",
    "            areay = disty * r * np.cos(thetay) / 2.\n",
    "            return sector(r, distx, theta=thetax) + sector(r, disty, theta=thetay) + \\\n",
    "                            sector(r, r, theta=np.pi / 2.) + distx * disty + areax + areay\n",
    "        else:\n",
    "            return np.pi * r**2 - segment(r, distx) - segment(r, disty)\n",
    "    else:\n",
    "        if vb: print('this case should not happen because we did not consider radii \\\n",
    "                beyond half the shortest side of the footprint,\\\n",
    "                but will at some point deal with this case')\n",
    "        return None\n",
    "\n",
    "\n",
    "# ## Calculates volume normalized environment\n",
    "# def calc_env(ind):\n",
    "#     \"\"\"\n",
    "#         Runs galenv to calculate galaxy environment.\n",
    "#         This is set up to run in the multiprocessing so a lot of inputs are\n",
    "#         not set when you call the function, but are supposed to be defined\n",
    "#         when running this code.\n",
    "\n",
    "#         Output: nearest neighbors at a given angular separation.\n",
    "#     \"\"\"\n",
    "#     if opts.run_environment:\n",
    "#         # Generates environments for GAMA RA/Dec data\n",
    "#         res = [subsamples[f][s]['CATAID'].values[ind]]\n",
    "#     if opts.run_particle_environment:\n",
    "#         # Generates environments for particle RA/Dec data\n",
    "#         res = [ind]\n",
    "\n",
    "#     friends = data\n",
    "#     for dist in try_distances:\n",
    "#         friends = galenv.nn_finder(friends, data[ind], dist)\n",
    "#         #print('r/dist', dist, 'x ', data[ind][0], 'y ', data[ind][1], 'minx', minx, 'maxx', maxx, 'miny', miny, 'maxy', maxy)\n",
    "#         vol = area(dist, data[ind][0], data[ind][1], minx, maxx, miny, maxy, vb=False)\n",
    "#         #print('vol', vol)\n",
    "#         res.append(float(len(friends)) / vol)\n",
    "#     return res\n",
    "# ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to select by redshift:\n",
    "# easiest to just go +/- 0.1\n",
    "# then get more sophisticated\n",
    "\n",
    "def give_redshift_neighbors(ind, delta=0.1):\n",
    "    my_neighborhood = the_neighborhood[ind][-1]\n",
    "    all_neighbor_zs = my_neighborhood['Z']\n",
    "    my_z = all_neighbor_zs[ind]\n",
    "    close_neighbor_dists = my_neighborhood['dist'+str(ind)][(all_neighbor_zs > my_z-delta) & (all_neighbor_zs < my_z+delta)]\n",
    "    return(close_neighbor_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in volume normalization here\n",
    "\n",
    "def help_make_curve(ind):\n",
    "    curve = [the_neighborhood[ind][1]]#np.empty((ndist))\n",
    "    (ra, dec) = (df.iloc[ind]['RA'], df.iloc[ind]['DEC'])\n",
    "#     redshifts = test_data[ind][3]\n",
    "    distances = sorted(list(give_redshift_neighbors(ind)))\n",
    "    for dist in distance_evaluation_points:\n",
    "#         print(i)\n",
    "        pos = bisect.bisect(distances, dist)\n",
    "#         print(pos)\n",
    "#         curve[i] = pos\n",
    "#         all_curves.iloc[ind][str(i)] = pos\n",
    "        vol = area(dist, ra, dec, minx, maxx, miny, maxy, vb=False)\n",
    "        if type(vol) != np.float64:\n",
    "            print(str((ind, pos))+' at '+str((ra, dec))+' with dist '+str(distances[pos])+' has area '+str(vol)+' of type '+str(type(vol)))\n",
    "        curve.append(float(pos) / vol)\n",
    "    return(curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nps = 20#mp.cpu_count() - 1\n",
    "pool = mp.Pool(nps)\n",
    "try_neighbor_curves = pool.map(help_make_curve, df.index)\n",
    "newdist = np.hstack((np.ones((1)), distance_evaluation_points))\n",
    "save_curves = np.vstack((newdist, try_neighbor_curves))\n",
    "pkl.dump(np.array(save_curves), open(toppath+'flexible_envirocurves/zslice_normed_log100dist_'+test_key+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_curves = pkl.load(open(toppath+'flexible_envirocurves/zslice_normed_curves_'+test_key+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do something smarter for getting borders\n",
    "\n",
    "minx = 174.\n",
    "maxx = 186.\n",
    "miny = -3.\n",
    "maxy = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vols = np.empty((ndist))\n",
    "# for i, dist in enumerate(distance_evaluation_points):\n",
    "#     vols[i] = area(dist, ra, dec, minx, maxx, miny, maxy, vb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also, subsample redshifts of neighbors to be near redshift of galaxy in question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next steps\n",
    "- [X] collect neighbors and distances for rudimentary curve\n",
    "- [X] variable angular distances for evaluation\n",
    "- [X] normalize for area enclosed within radius\n",
    "- [X] redshift/depth cutting\n",
    "- [ ] physical distance conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_curves = pd.DataFrame(columns = distance_evaluation_points).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gal = len(np.unique(test[0]))\n",
    "sparse_arr = np.empty((n_gal, n_gal))\n",
    "sparse_arr[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_sparse(ind):\n",
    "    x, y = test[0][ind], test[1][ind]\n",
    "    sparse_arr[x][y] = test[2][ind].value\n",
    "    return sparse_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for ind in range(len(test[0])):\n",
    "#     fill_sparse(ind)\n",
    "n_pair = len(test[0])\n",
    "nps = 15\n",
    "pool = mp.Pool(nps)\n",
    "pool.map(fill_sparse, range(n_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df = pd.DataFrame(sparse_arr).astype(pd.SparseDtype(\"float\", np.nan))\n",
    "sparse_df.to_csv('sparse_neighbors_G12.csv')\n",
    "sparse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_test = pd.DataFrame(sparse_arr).astype(pd.SparseDtype(\"float\", np.nan))\n",
    "sparse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_inds['G15']['neighbor_info'] = \n",
    "easy_inds['G15']['neighbor_ids'] = []\n",
    "def fetch_neighbors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in gf.keys():\n",
    "    small_piece = gf[field]\n",
    "    just_ids = small_piece['CATAID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_neighbors(field):\n",
    "    near_dists = pkl.load(open(toppath+'flexible_envirocurves/neighbors_'+field+'_allz.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array([output[0], output[1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_q.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(all_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(high_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords1 = ap.coordinates.SkyCoord(small_piece['RA'], small_piece['DEC'], unit='deg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_piece = df.sample(200)\n",
    "# coords2 = ap.coordinates.SkyCoord(small_piece['RA'], small_piece['DEC'], unit='deg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(coords1.ra, coords1.dec)\n",
    "# plt.scatter(coords2.ra, coords2.dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # warning: slow!!! (not actually that slow)\n",
    "# output = ap.coordinates.search_around_sky(coords1, coords1, maxang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for o in output:\n",
    "#     print((len(o), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(output[2].value, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(output[0], bins=len(np.unique(output[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recidivator (Python 3)",
   "language": "python",
   "name": "recidivator_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
