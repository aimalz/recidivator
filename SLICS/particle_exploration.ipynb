{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the __SLICS-HR__ particle data\n",
    "notebook by _Alex Malz (GCCL@RUB)_, (add your name here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import treecorr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the precomputed 2PCF\n",
    "\n",
    "Download the 2PCF at several redshifts [here](https://drive.google.com/drive/folders/1eGlAO_wl9h0xiXiTMKV_m7h9YCRhDHP_?usp=sharing).\n",
    "\n",
    "Note that the data is $\\Delta^{2}(k)$, not the more familiar (to me) $\\mathcal{P}(k)$.  (A reminder of the relationship between them can be found [here](http://universe-review.ca/R05-04-powerspectrum.htm), particularly in [this figure](http://universe-review.ca/I02-20-correlate1b.png).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_snapshots = np.array(['0.042', '0.130', '0.221', '0.317', '0.418', '0.525', '0.640', '0.764', '0.897', '1.041'])\n",
    "pk = []\n",
    "for i in range(len(z_snapshots)):\n",
    "    fn = z_snapshots[i]+'ngpps_new.dat_LOS1'\n",
    "    file_loc = os.path.join('NptFns', fn)\n",
    "    pk.append(np.genfromtxt(file_loc).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(z_snapshots)):\n",
    "    plt.plot(pk[i][0], pk[i][1], label=z_snapshots[i])\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(r'$k$ [Mpc/h]')\n",
    "plt.ylabel(r'$\\Delta^2(k)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "\n",
    "Download one of the 64 nodes $\\times$ 20 redshifts files at each redshift from Joachim Harnois-Deraps to start.\n",
    "I chose file 27 at $z=0.042$ for this example.\n",
    "Read in from binary float(4) format and throw out first 12 entries as unwanted header information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_str = '0.042'\n",
    "fn_base = 'xv'\n",
    "fn_index = 27\n",
    "fn_ext = '.dat'\n",
    "fn = z_str + fn_base + str(fn_index) + fn_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_each = 'f' + str(4)\n",
    "dt = np.dtype([('x', dt_each), ('y', dt_each), ('z', dt_each), ('vx', dt_each), ('vy', dt_each), ('vz', dt_each)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn, 'rb') as f1:\n",
    "    raw_data = np.fromfile(f1, dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data = pd.DataFrame(data=raw_data[2:], columns=['x', 'y', 'z', 'vx', 'vy', 'vz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ('x', 'y', 'z'):\n",
    "    plt.hist(loc_data[i], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to physical units\n",
    "\n",
    "The particle data starts out in simulation units relative to the per-node subvolume and needs to be converted to physical units in the space of all subvolumes before the whole volume can be considered.\n",
    "Note that the conversion below makes sense for `x`, `y`, and `z` but not for `vx`, `vy`, and `vz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of MPI tasks per dimension\n",
    "nodes_dim = 4\n",
    "\n",
    "# subvolume size\n",
    "ncc = 768\n",
    "\n",
    "# volume size\n",
    "rnc = 3072."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k1 in range(1, nodes_dim+1):\n",
    "    for j1 in range(1, nodes_dim+1):\n",
    "        for i1 in range(1, nodes_dim+1):\n",
    "            if fn_index == (i1 - 1) + (j1 - 1) * nodes_dim + (k1 - 1) * nodes_dim ** 2:\n",
    "                print('found index '+str(fn_index)+' at '+str((i1, j1, k1)))\n",
    "                node_coords = {'x': i1 - 1, 'y': j1 - 1, 'z': k1 - 1}\n",
    "                print(node_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift data\n",
    "glob_data = loc_data\n",
    "for col in ['x', 'y', 'z']:\n",
    "    glob_data[col] = np.remainder(loc_data[col] + node_coords[col] * ncc, rnc)\n",
    "    assert(max(glob_data[col] <= rnc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Mpc/h\n",
    "phys_data = glob_data * 505. / 3072."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data to determine reasonable limits in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ('x', 'y', 'z'):\n",
    "    plt.hist(phys_data[i], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data for [`treecorr`](https://github.com/rmjarvis/TreeCorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_data.to_csv('for_treecorr.csv', columns=['x', 'y'], header=False, index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining results from multiple files\n",
    "\n",
    "Calculating the 3-point function on even one of the 64 files is quite slow, so running it on all the files at a given redshift in one go may not be feasible.\n",
    "However, since we anticipate getting error bars for $\\zeta$, we can take advantage of [sampling theory](https://web.stanford.edu/class/archive/lectureHandouts/170-samples.pdf) to combine the 3-point correlation functions of each subsample.\n",
    "\n",
    "$\\langle\\bar{\\zeta}\\rangle = \\langle\\dots\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Ultimately, we will need to calculate the 2 and 3+ point correlation functions of the particle data.\n",
    "Because the data is split into 64 files per redshift, we also need a way to combine the positional information from each file to get coherent correlation functions.\n",
    "We may be able to more easily accomplish both goals if we first smooth the data using a Fourier-space basis like wavelets.\n",
    "\n",
    "## combine particle data from multiple files\n",
    "\n",
    "## calculate the N-point correlation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COIN (Python 3)",
   "language": "python",
   "name": "coin_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
