{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~Exploring~~ Preparing the __SLICS-HR__ particle data\n",
    "notebook by _Alex Malz (GCCL@RUB)_, (add your name here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy as ap\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're only considering the first 8 SLICS snapshots because the GAMA data doesn't have good enough coverage beyond that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_SLICS = np.array([0.042, 0.080, 0.130, 0.221, 0.317, 0.418, 0.525])\n",
    "#, 0.640, 0.764, 0.897, 1.041, 1.199, 1.372, 1.562, 1.772, 2.007, 2.269, 2.565, 2.899])\n",
    "z_mids = (z_SLICS[1:] + z_SLICS[:-1]) / 2.\n",
    "z_bins = np.insert(z_mids, 0, 0.023)\n",
    "z_bins = np.append(z_bins, 3.066)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of MPI tasks per dimension\n",
    "nodes_dim = 4\n",
    "\n",
    "# volume size\n",
    "rnc = 3072.\n",
    "\n",
    "# subvolume size\n",
    "ncc = rnc / nodes_dim\n",
    "\n",
    "# physical scale in Mpc/h\n",
    "phys_scale = 505."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much data do we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SLICS cosmology has $\\Omega_{m} = 0.2905$, $\\Omega_{\\Lambda} = 0.7095$, $\\Omega_{b} = 0.0473$, $h = 0.6898$, $\\sigma_{8} = 0.826$, and $n_{s} = 0.969$.\n",
    "Let's assume the naive Cartesian-to-angular coordinates and flatten along the `z` direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxdepth = 500.e6 / 4. * ap.units.pc\n",
    "# boxdepth per file, not per redshift integrated!\n",
    "\n",
    "h = 0.6898\n",
    "cosmo = FlatLambdaCDM(H0=100.*h, Om0=0.2905, Ob0=0.0473)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line-of-sight integration depth\n",
    "\n",
    "We need to ensure a correspondance between the redshift range of the GAMA data about each $z_{SLICS}$ and the depth of SLICS data to integrate over when projecting the 3D density into 2D sky coordinates.\n",
    "For simplicity, let's do this along one of the Cartesian axes of the SLICS data even though it can justifiably be done along any line of sight projection.\n",
    "It doesn't matter which is chosen first between the GAMA redshift bin ends and the SLICS projection depth, but life is easier if we don't have to combine multiple SLICS files, meaning that depth should be $depth_{physical} \\leq \\frac{1}{4} \\times 500 Mpc/h$.\n",
    "In fact, it would be easiest to take $depth_{physical} = \\frac{1}{4} \\times 500 Mpc/h$.\n",
    "The next step would be to calculate the redshift bin endpoints for GAMA under the SLICS cosmology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_comov = []\n",
    "for z in z_SLICS:\n",
    "    dc = cosmo.comoving_distance(float(z))\n",
    "    d_comov.append(dc.value / h)\n",
    "d_comov = np.array(d_comov) * 1e6 * ap.units.pc\n",
    "\n",
    "d_comov_mins = d_comov - boxdepth / 2.\n",
    "d_comov_maxs = d_comov + boxdepth / 2.\n",
    "min_zs = [ap.cosmology.z_at_value(cosmo.comoving_distance, d_comov_min * h) for d_comov_min in d_comov_mins]\n",
    "max_zs = [ap.cosmology.z_at_value(cosmo.comoving_distance, d_comov_max * h) for d_comov_max in d_comov_maxs]\n",
    "# print(list(zip(min_zs, max_zs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### area\n",
    "\n",
    "Area will be determined by the maximum distance over which colors are correlated.\n",
    "We expect the characteristic scale to not be significantly larger than a galaxy cluster, $diameter \\leq 10 Mpc$.\n",
    "For now, let's use a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmax = 10.e6 * ap.units.pc\n",
    "rmax = dmax / h / 2.\n",
    "d_ang = (d_comov_maxs + d_comov_mins) / 2. / (1 + z_SLICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### particle density field bubble locations\n",
    "\n",
    "We only need mock galaxy positions corresponding to areas over which galaxy colors are correlated.\n",
    "Let's assume we have this function $radius(z)$ evaluated at each $z_{SLICS}$ where there is sufficient GAMA data.\n",
    "Further, let's assume that $radius(z) \\ll \\frac{1}{4} \\times 500 Mpc/h$ so we don't have to combine multiple SLICS files or worry about boundaries within SLICS files.\n",
    "We can choose to center our cylinders of data where the projected density field is highest, so a first step is to identify some such points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in from binary float(4) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_each = 'f' + str(4)\n",
    "dt = np.dtype([('x', dt_each), ('y', dt_each), ('z', dt_each), ('vx', dt_each), ('vy', dt_each), ('vz', dt_each)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubeside = rnc / nodes_dim\n",
    "dmin = cubeside * rmax / (phys_scale * 1.e6 * ap.units.pc / nodes_dim)\n",
    "rmin = dmin / 2.\n",
    "resolution = (phys_scale * 1.e6 * ap.units.pc / nodes_dim) / rmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throw out first 12 entries as unwanted header information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_read(which_z, fn_index):\n",
    "    z_str = '{:<05}'.format(str(z_SLICS[which_z]))\n",
    "    fn_base = 'xv'\n",
    "    fn_ext = '.dat'\n",
    "    fn = z_str + fn_base + str(fn_index) + fn_ext\n",
    "    data_dir = 'particle_data/cuillin.roe.ac.uk/~jharno/SLICS/SLICS_HR/LOS1'\n",
    "    with open(os.path.join(data_dir, fn), 'rb') as f1:\n",
    "        raw_data = np.fromfile(f1, dtype=dt)\n",
    "    if loc_data.duplicated().any():\n",
    "        print('duplicates found in z='+str(z_SLICS[which_z])+' box='+str(fn_index)+'!')\n",
    "    loc_data = pd.DataFrame(data=raw_data[2:], columns=['x', 'y', 'z', 'vx', 'vy', 'vz']).drop_duplicates()\n",
    "    assert(~loc_data.duplicated().any())\n",
    "    return(loc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, throwing out duplicates is slow (why???), but it really should be done before the coarse histogram so has to happen on the whole file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: combine z coordinates over multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testz = 2\n",
    "# testfn = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc_data = help_read(which_z=testz, fn_index=testfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to select in the native coordinates of the data so we only perform the conversion to physical coordinates for the portions of data we're going to use.\n",
    "The following cell is a bit slow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (coarsedensity, xedges, yedges) = np.histogram2d(loc_data['x'], loc_data['y'], bins=int(resolution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: finedensity to pick sane bubble centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(coarsedensity.flatten(), bins=int(resolution))\n",
    "# plt.semilogx()\n",
    "# plt.savefig('densitydist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme = np.quantile(coarsedensity.flatten(), 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indcenters = np.argwhere(coarsedensity > extreme)\n",
    "# xcenters = (xedges[indcenters.T[0]] + xedges[indcenters.T[0]+1]) / 2\n",
    "# ycenters = (yedges[indcenters.T[1]] + yedges[indcenters.T[1]+1]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(5, 5))\n",
    "# ax = fig.add_subplot(111, title='projected histogram', aspect='equal')\n",
    "# X, Y = np.meshgrid(xedges, yedges)\n",
    "# ax.pcolormesh(X, Y, np.log(coarsedensity))\n",
    "# plt.scatter(ycenters, xcenters, c='r', s=10, alpha=0.5)\n",
    "# fig.savefig('loghistogram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bubbles = []\n",
    "# for i, center in enumerate(indcenters):\n",
    "#     bubble = loc_data.loc[lambda df: (df['x'] > xcenters[i] - rmin) & (df['x'] < xcenters[i] + rmin)\n",
    "#                           & (df['y'] > ycenters[i] - rmin) & (df['y'] < ycenters[i] + rmin), :]\n",
    "#     assert(~bubble.duplicated().any())\n",
    "#     bubbles.append(bubble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist2d(loc_data['x'], loc_data['y'], bins=(200, 200), norm=mpl.colors.LogNorm(), cmap='Spectral_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc_to_plot = loc_data.sample(50000)\n",
    "# plt.scatter(loc_to_plot['x'], loc_to_plot['y'], marker='.', s=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to physical units\n",
    "\n",
    "The particle data starts out in simulation units relative to the per-node subvolume and needs to be converted to physical units in the space of all subvolumes before the whole volume can be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the conversion below makes sense for `x`, `y`, and `z` but not for `vx`, `vy`, and `vz`.\n",
    "Because of how the data is distributed across the files, I think 21, 22, 25, 26, 37, 38, 41, 42 are \"adjacent\" and free of edge effects.\n",
    "_Note_: We can just have this be an automated check, knowing that files are adjacent when their `node_coords` are the same aside from being off by one in one of their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_find_coords(fn_index):\n",
    "#     all_nodes_coords = np.empty((nodes_dim, nodes_dim, nodes_dim))\n",
    "    for k1 in range(1, nodes_dim+1):\n",
    "        for j1 in range(1, nodes_dim+1):\n",
    "            for i1 in range(1, nodes_dim+1):\n",
    "                current_ind = (i1 - 1) + (j1 - 1) * nodes_dim + (k1 - 1) * nodes_dim ** 2\n",
    "                node_coords = {'x': i1 - 1, 'y': j1 - 1, 'z': k1 - 1}\n",
    "                if fn_index == current_ind:\n",
    "#                     print('found index '+str(fn_index)+' at '+str((i1, j1, k1)))\n",
    "                    true_node_coords = node_coords\n",
    "#                 all_nodes_coords[node_coords['x'], node_coords['y'], node_coords['z']] = current_ind\n",
    "                    return(true_node_coords)\n",
    "# print(all_nodes_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_node_coords = help_find_coords(fn_index=testfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get coherent coordinates across all files, we need to shift them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_shift(true_node_coords, loc_data):\n",
    "    # shift data\n",
    "    glob_data = loc_data\n",
    "    for col in ['x', 'y', 'z']:\n",
    "        glob_data[col] = np.remainder(loc_data[col] + true_node_coords[col] * ncc, rnc)\n",
    "        assert(max(glob_data[col] <= rnc))\n",
    "    return(glob_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globs = []\n",
    "# for loc_data in bubbles:\n",
    "#     glob_data = help_shift(loc_data)\n",
    "#     globs.append(glob_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_convert(glob_data):\n",
    "    # convert to Mpc/h\n",
    "    phys_data = glob_data * phys_scale / rnc\n",
    "    return(phys_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_mpc = []\n",
    "# for glob_data in globs:\n",
    "#     phys_data = help_convert(glob_data)\n",
    "#     pos_mpc.append(phys_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dim in ['x', 'y', 'z']:\n",
    "#     plt.hist(phys_data[dim], density=True, alpha=0.5)\n",
    "# plt.xlabel('distance (Mpc/h)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist2d(phys_data['x'], phys_data['y'], bins=(200,200), norm=mpl.colors.LogNorm(), cmap='Spectral_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phys_to_plot = phys_data.sample(50000)\n",
    "# plt.scatter(phys_to_plot['x'], phys_to_plot['y'], marker='.', s=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert physical units to angular units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_ang = []\n",
    "# for phys_data in pos_mpc:\n",
    "#     ang_data = pd.DataFrame()\n",
    "#     ang_data['RA'] = phys_data['x']# / d_ang.value[testz] * 180. / np.pi\n",
    "#     ang_data['DEC'] = phys_data['y']# / d_ang.value[testz] * 180. / np.pi\n",
    "#     pos_ang.append(ang_data.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, len(pos_ang), figsize=(5*len(pos_ang), 5))\n",
    "# for i, pos in enumerate(pos_ang):\n",
    "#     axs[i].scatter(pos['RA'], pos['DEC'], s=0.1, c='k', alpha=0.1)\n",
    "# # plt.savefig('bubbles.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, pos in enumerate(pos_ang):\n",
    "#     pos.to_csv('../DEAR/Data/bubbles/z'+str(testz)+'box'+str(testfn)+'bubble'+str(i)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These still look like they're cutting through the high-density regions, so I'm going to add an intermediate step to pick the bubble centers on a fine grid evaluated only within the vicinity of the densest of the coarse grid centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, as a parallelized pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble_combos = {}\n",
    "for i in [2]:#range(len(z_SLICS)):\n",
    "    for j in [22]:#, 22, 25, 26, 37, 38, 41, 42]:\n",
    "        #Note: j loop will change a lot when SLICS files are combined\n",
    "        bubble_combos['z'+str(i)+'box'+str(j)] = (i, j)\n",
    "        \n",
    "pathname = '../DEAR/Data/bubbles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_one_bubble(one_key):\n",
    "    (testz, testfn) = bubble_combos[one_key]\n",
    "    print('starting z='+str(z_SLICS[testz]))\n",
    "    zpath = pathname+'z'+str(testz)\n",
    "    if os.path.isdir(zpath) == False:\n",
    "        os.system('mkdir '+ zpath)\n",
    "    boxpath = zpath+'/box'+str(testfn)\n",
    "    if os.path.isdir(boxpath) == False:\n",
    "        os.system('mkdir '+ boxpath)\n",
    "    elif os.listdir(boxpath) != []:\n",
    "        print('not-rerunning z='+str(z_SLICS[testz])+' box='+str(testfn))\n",
    "        return None\n",
    "    #could use this as a place to not have to reload SLICS data file, instead save bubble before doing finedensity\n",
    "    print('starting box='+str(testfn)+', loading SLICS data (the slow step)')\n",
    "    loc_data = help_read(which_z=testz, fn_index=testfn)\n",
    "    print('loaded SLICS data, identifying bubble centers')\n",
    "    (coarsedensity, xedges, yedges) = np.histogram2d(loc_data['x'], loc_data['y'], bins=int(resolution))\n",
    "    extreme = np.quantile(coarsedensity.flatten(), 0.99)\n",
    "    indcenters = np.argwhere(coarsedensity > extreme)\n",
    "    #finedensity step would go here\n",
    "    xcenters = (xedges[indcenters.T[0]] + xedges[indcenters.T[0]+1]) / 2\n",
    "    ycenters = (yedges[indcenters.T[1]] + yedges[indcenters.T[1]+1]) / 2\n",
    "    print('identified bubble centers, going through each bubble')\n",
    "#     bubbles, globs, pos_mpc, pos_ang = [], [], [], []\n",
    "    for i, center in enumerate(indcenters):\n",
    "        bubpath = boxpath+'/bub'+str(i)\n",
    "        if os.path.isdir(bubpath) == False:\n",
    "            os.system('mkdir '+ bubpath)\n",
    "        bubble = loc_data.loc[lambda df: (df['x'] > xcenters[i] - rmin) & (df['x'] < xcenters[i] + rmin)\n",
    "                          & (df['y'] > ycenters[i] - rmin) & (df['y'] < ycenters[i] + rmin), :]\n",
    "        if ~bubble.duplicated().any():\n",
    "            print('no duplicate coordinates in x, y, z, vx, vy, vz')\n",
    "        bubble.to_csv(bubpath+'/particles.csv', index=False)\n",
    "        print('saved bubble particles to not have to load whole SLICS file again, next transform data')\n",
    "#         bubbles.append(bubble)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nps = mp.cpu_count() - 1\n",
    "pool = mp.Pool(nps)\n",
    "pool.map(isolate_one_bubble, bubble_combos.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_one_bubble(one_key):\n",
    "    (testz, testfn) = bubble_combos[one_key]\n",
    "    true_node_coords = help_find_coords(fn_index=testfn)\n",
    "    zpath = pathname+'z'+str(testz)\n",
    "    boxpath = zpath+'/box'+str(testfn)\n",
    "    bubpaths = os.listdir(boxpath)\n",
    "    fullpaths = [boxpath+'/'+bubpath+'/' for bubpath in bubpaths]\n",
    "    for fullpath in fullpaths:\n",
    "        bubble = pd.read_csv(fullpath+'particles.csv')\n",
    "        glob_data = help_shift(true_node_coords, bubble)\n",
    "#         globs.append(glob_data)\n",
    "        phys_data = help_convert(glob_data)\n",
    "#         pos_mpc.append(phys_data)\n",
    "        ang_data = pd.DataFrame()\n",
    "        ang_data['RA'] = phys_data['x'] / d_ang.value[testz] * 180. / np.pi\n",
    "        ang_data['DEC'] = phys_data['y'] / d_ang.value[testz] * 180. / np.pi\n",
    "        if bubble.duplicated().any():\n",
    "            print('duplicate particles introduced by dropping z, vx, vy, vz')\n",
    "#         if bubble.duplicated().any():\n",
    "#             print('dropped duplicates')\n",
    "#             to_save = ang_data.drop_duplicates()\n",
    "#         pos_ang.append(ang_data.drop_duplicates())\n",
    "        #\n",
    "        ang_data.to_csv(fullpath+'projection.csv', index=False)\n",
    "        print('shifted from machine, converted to physical, projected to angular coordinates, and saved '+str(len(ang_data)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nps = mp.cpu_count() - 1\n",
    "pool = mp.Pool(nps)\n",
    "pool.map(project_one_bubble, bubble_combos.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: start RDEAR process with jittering duplicates if any introduced by dropping z, vx, vy, vz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attic below here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much data do we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much depth?\n",
    "\n",
    "Obtain necessary depth from ~~[Ned Wright's cosmology calculator](http://www.astro.ucla.edu/~wright/CosmoCalc.html)~~ `astropy`.\n",
    "The SLICS cosmology has $\\Omega_{m} = 0.2905$, $\\Omega_{\\Lambda} = 0.7095$, $\\Omega_{b} = 0.0473$, $h = 0.6898$, $\\sigma_{8} = 0.826$, and $n_{s} = 0.969$.\n",
    "Let's assume the naive Cartesian-to-angular coordinates and flatten along the `z` direction.\n",
    "We need to flatten a depth corresponding to the bounds of each redshift bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.6898\n",
    "cosmo = FlatLambdaCDM(H0=100.*h, Om0=0.2905, Ob0=0.0473)\n",
    "d_comov = []\n",
    "for z in z_bins:\n",
    "    dc = cosmo.comoving_distance(float(z))\n",
    "    d_comov.append(dc.value / h)\n",
    "d_comov = np.array(d_comov)\n",
    "depths = d_comov[1:] - d_comov[:-1]\n",
    "\n",
    "avg_d_comov = []\n",
    "for z in z_SLICS:\n",
    "    dc = cosmo.comoving_distance(float(z))\n",
    "    avg_d_comov.append(dc.value / h)\n",
    "    \n",
    "print(depths)\n",
    "print(avg_d_comov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_z = np.ceil(depths / phys_scale)\n",
    "print(n_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, `depths` < `phys_scale` $Mpc/h$ only in the first three redshift bins, meaning the depths of the next three GAMA redshift bin may require opening two files, and the last of these bins cannot be turned into a mock catalog based on SLICS.\n",
    "I think the way they're arranged means that (21, 37), (22, 38), (25, 41), and (26, 42) are pairs adjacent in `z`.\n",
    "\n",
    "_This is as good a time as any to note that our mock catalog will have a bit of a degeneracy if we use the same file numbers for all redshifts because each file corresponds to the same physical volume across cosmic time, whereas in a real survey, our redshift bins contain different volumes/galaxies.\n",
    "We have a choice to make about discontinuities or non-physical repetitition._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much area?\n",
    "\n",
    "Obtain angular diameter distance $d_{a}$ in units $\\theta = x / d_{a}$ with $d_{a} = d_{c} / (1 + z)$, where $d_{c}$ is the comoving diameter distance and $x$ is the distance in the SLICS data.\n",
    "Compare with the GAMA footprint of $286^{\\circ^{2}} * (\\pi / 180^{\\circ})^{2} \\approx 0.087 sr$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ang = avg_d_comov / (1 + z_SLICS)\n",
    "theta_box = phys_scale / d_ang * 180. / np.pi\n",
    "footprint = theta_box**2\n",
    "print(footprint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling behavior is as expected;\n",
    "`phys_scale` subtends a larger angle at low redshifts and a smaller angle at high redshifts.\n",
    "One file's worth of SLICS data subtends an angular area larger than the GAMA footprint in the first five GAMA redshift bins, but the next three GAMA redshift bins would definitely require more than one file's worth of data.\n",
    "We need to pick an angular area for our mock galaxy catalog.\n",
    "Let's go with twice that for now.\n",
    "~~_Do we think twice the GAMA area is sufficiently compelling?_~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_gama = 286.\n",
    "GAMA_phys_scale = np.sqrt(theta_gama) * (np.pi / 180.) * d_ang\n",
    "print(GAMA_phys_scale**2 / 505**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_mock = 2. * theta_gama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many files are needed to fill out the RA/DEC space for a mock survey of twice the GAMA area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_xy = np.ceil(theta_gama / footprint)\n",
    "# n_xy = np.empty(len(z_SLICS))\n",
    "# one_xy = np.where(footprint > theta_mock)[0]\n",
    "# n_xy[one_xy] = 1\n",
    "# i = 1\n",
    "# while i <= 64:\n",
    "#     which_xy = np.where((i * footprint < theta_mock) & ((i+1) * footprint > theta_mock))[0]\n",
    "#     n_xy[which_xy] = i+1\n",
    "#     i += 1\n",
    "print(n_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go with twice the GAMA footprint, then the first four redshift bins need only one file but the next two need 2, and the two after that need 3 and 4.\n",
    "I think (21, 22), (25, 26), (37, 38), and (41, 42) are adjacent in `x`/`RA` and (21, 25), (22, 26), (37, 41), and (38, 42) are adjacent in `y`/`DEC`.\n",
    "However, we'll have to open the files that approach the edges for the last two redshifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the limiting size in RA/DEC, and to skip a time-consuming conversion of the data, will convert that into x/y at each redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_omega = 2 * theta_gama\n",
    "lim_a = lim_omega * (np.pi / 180.* d_ang)**2\n",
    "\n",
    "lim_theta = np.sqrt(lim_omega)\n",
    "lim_xy =  lim_theta * np.pi / 180.* d_ang\n",
    "\n",
    "print(sum(lim_a / phys_scale**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use squares with a shared origin for now, and if at some point if we really want non-overlapping footprints, we know we'll need the area of 11 out of 16 files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What it looks like for one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data\n",
    "\n",
    "Download one of the 64 nodes $\\times$ 20 redshifts files at each redshift from Joachim Harnois-Deraps to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in from binary float(4) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_each = 'f' + str(4)\n",
    "dt = np.dtype([('x', dt_each), ('y', dt_each), ('z', dt_each), ('vx', dt_each), ('vy', dt_each), ('vz', dt_each)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throw out first 12 entries as unwanted header information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_read(which_z, fn_index):\n",
    "    z_str = '{:<05}'.format(str(z_SLICS[which_z]))\n",
    "    fn_base = 'xv'\n",
    "    fn_ext = '.dat'\n",
    "    fn = z_str + fn_base + str(fn_index) + fn_ext\n",
    "    data_dir = 'particle_data/cuillin.roe.ac.uk/~jharno/SLICS/SLICS_HR/LOS1'\n",
    "    with open(os.path.join(data_dir, fn), 'rb') as f1:\n",
    "        raw_data = np.fromfile(f1, dtype=dt)\n",
    "    loc_data = pd.DataFrame(data=raw_data[2:], columns=['x', 'y', 'z', 'vx', 'vy', 'vz'])\n",
    "    return(loc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slow to read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data = help_read(which_z=2, fn_index=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist2d(loc_data['x'], loc_data['y'], bins=(200, 200), norm=mpl.colors.LogNorm(), cmap='Spectral_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(loc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc_to_plot = loc_data.sample(50000)\n",
    "# plt.scatter(loc_to_plot['x'], loc_to_plot['y'], marker='.', s=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to physical units\n",
    "\n",
    "The particle data starts out in simulation units relative to the per-node subvolume and needs to be converted to physical units in the space of all subvolumes before the whole volume can be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the conversion below makes sense for `x`, `y`, and `z` but not for `vx`, `vy`, and `vz`.\n",
    "Because of how the data is distributed across the files, I think 21, 22, 25, 26, 37, 38, 41, 42 are \"adjacent\" and free of edge effects.\n",
    "_Note_: We can just have this be an automated check, knowing that files are adjacent when their `node_coords` are the same aside from being off by one in one of their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_find_coords(fn_index):\n",
    "#     all_nodes_coords = np.empty((nodes_dim, nodes_dim, nodes_dim))\n",
    "    for k1 in range(1, nodes_dim+1):\n",
    "        for j1 in range(1, nodes_dim+1):\n",
    "            for i1 in range(1, nodes_dim+1):\n",
    "                current_ind = (i1 - 1) + (j1 - 1) * nodes_dim + (k1 - 1) * nodes_dim ** 2\n",
    "                node_coords = {'x': i1 - 1, 'y': j1 - 1, 'z': k1 - 1}\n",
    "                if fn_index == current_ind:\n",
    "#                     print('found index '+str(fn_index)+' at '+str((i1, j1, k1)))\n",
    "                    true_node_coords = node_coords\n",
    "#                 all_nodes_coords[node_coords['x'], node_coords['y'], node_coords['z']] = current_ind\n",
    "                    return(true_node_coords)\n",
    "# print(all_nodes_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_node_coords = help_find_coords(fn_index=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get coherent coordinates across all files, we need to shift them accordingly.\n",
    "The next cell is unexpectely slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_shift(loc_data):\n",
    "    # shift data\n",
    "    glob_data = loc_data\n",
    "    for col in ['x', 'y', 'z']:\n",
    "        glob_data[col] = np.remainder(loc_data[col] + true_node_coords[col] * ncc, rnc)\n",
    "        assert(max(glob_data[col] <= rnc))\n",
    "    return(glob_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_data = help_shift(loc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_convert(glob_data):\n",
    "    # convert to Mpc/h\n",
    "    phys_data = glob_data * phys_scale / rnc\n",
    "    return(phys_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_data = help_convert(glob_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dim in ['x', 'y', 'z']:\n",
    "#     plt.hist(phys_data[dim], density=True, alpha=0.5)\n",
    "# plt.xlabel('distance (Mpc/h)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist2d(phys_data['x'], phys_data['y'], bins=(200,200), norm=mpl.colors.LogNorm(), cmap='Spectral_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phys_to_plot = phys_data.sample(50000)\n",
    "# plt.scatter(phys_to_plot['x'], phys_to_plot['y'], marker='.', s=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try automating it\n",
    "\n",
    "Note: We are not attempting to enforce the anisotropy of the sky, i.e. the particles in file number X are by and large the same particles at each redshift because they're in comoving coordinates.\n",
    "In contrast, when we observe the sky, we don't see the same galaxies evolved to different redshifts but instead see different, coherent portions of the large-scale structure at different redshifts.\n",
    "This is something we can address later, but I'm trying to keep it simple-ish for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chop up or combine data files as needed\n",
    "This is slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_depth(which_z, filenos):\n",
    "    i = 0\n",
    "    while i < 4:\n",
    "        temp = np.mod(phys_data['z'] - min(phys_data['z']), phys_scale)\n",
    "    return(phys_data[np.mod(phys_data['z'] - min(phys_data['z']), phys_scale) < depths[which_z]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ang_data = right_depth(phys_data, which_z)#phys_data[np.mod(phys_data['z'] - min(phys_data['z']), phys_scale) < depths[which_z]]\n",
    "ang_data['RA'] = ang_data['x'] / d_ang[which_z] * 180. / np.pi\n",
    "ang_data['DEC'] = ang_data['y'] / d_ang[which_z] * 180. / np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd change this for the area of our mock survey when we decide on it.\n",
    "_There is an edge effect going on right now.\n",
    "I need to switch to one of the internal files to avoid roll-over that's breaking min/max checks._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_data = phys_data[(phys_data['x'] < lim_xy + min(phys_data['x'])) & (phys_data['y'] < lim_xy + min(phys_data['y']))]\n",
    "#ang_data[(ang_data['RA'] < lim_theta + min(ang_data['RA'])) & (ang_data['DEC'] < lim_theta + min(ang_data['DEC']))]\n",
    "\n",
    "# plt.hist(cut_data['RA'])\n",
    "# plt.hist(cut_data['DEC'])\n",
    "\n",
    "cut_data.to_csv(z_str+'cut.csv', header=True, index=False, sep=',', columns=['RA', 'DEC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist2d(cut_data['RA'], cut_data['DEC'], bins=(200,200), norm=mpl.colors.LogNorm(), cmap='Spectral_r')\n",
    "# plt.xlabel('RA (deg)')\n",
    "# plt.ylabel('DEC (deg)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut_to_plot = cut_data.sample(50000)\n",
    "# plt.scatter(cut_to_plot['x'], cut_to_plot['y'], marker='.', s=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch after here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatially subsample data\n",
    "\n",
    "Turns out 1/64th of the total data was still way more than we could reasonably use at once to compute correlation functions!\n",
    "This should really be sliced by size of box.\n",
    "First, just break it up into smaller boxes.\n",
    "Let's say we want $10^{5}$ particles per box, so we'll cut it in 16 in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     j = i+1\n",
    "#     subset = phys_data[(phys_data['x'] <= 10.*j) & (phys_data['y'] <= 10.*j) & (phys_data['z'] <= 10.*j)]\n",
    "#     subset.to_csv('spat'+str(j)+'0Mpc.csv', header=False, index=False, sep=' ', columns=['x', 'y', 'z'])\n",
    "#     angular = subset / 313.5 * 69.6 / 100. * float(j) * 180 / np.pi\n",
    "#     print((min(angular['x']), max(angular['x'])))\n",
    "#     print((min(angular['y']), max(angular['y'])))  \n",
    "#     angular.to_csv('ang'+str(j)+'deg.csv', header=False, index=False, sep=',', columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # distances = np.sqrt(phys_data['x']**2 + phys_data['y']**2 + phys_data['z']**2)\n",
    "# splitpoints = {}\n",
    "# for dim in ['RA', 'DEC']:\n",
    "#     splitpoints[dim] = np.linspace(min(ang_data[dim]), max(ang_data[dim]), 17)\n",
    "#     print(splitpoints[dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(16):\n",
    "#     for j in range(16):\n",
    "#         subsample = ang_data.loc[(ang_data['RA'] >= splitpoints['RA'][i]) & (ang_data['RA'] <= splitpoints['RA'][i+1])\\\n",
    "#                                  & (ang_data['DEC'] >= splitpoints['DEC'][j]) & (ang_data['DEC'] <= splitpoints['DEC'][j+1])]\n",
    "#         subsample.to_csv(z_str+'slice_'+str(i)+'_'+str(j)+'.csv', header=True, index=False, sep=',', columns=['RA', 'DEC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly subsample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(angular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_plot = angular.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist2d(to_plot['x'], to_plot['y'], bins=100, norm=mpl.colors.LogNorm(), cmap='Spectral_r')\n",
    "# plt.savefig('mock_gal_pos.png', dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_distances = np.flip(np.geomspace(0.01, 1.0, 10), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import environment as galenv\n",
    "\n",
    "# def calc_env(ind):\n",
    "#     res = []\n",
    "#     friends = data\n",
    "#     for dist in try_distances:\n",
    "#         friends = galenv.nn_finder(friends, data[ind], dist)\n",
    "#         res.append(len(friends))\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [to_plot['x'].values, to_plot['y'].values]\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.array([to_plot['x'].values, to_plot['y'].values]).T\n",
    "# nps = mp.cpu_count()\n",
    "# pool = mp.Pool(nps - 1)\n",
    "# envs = pool.map(calc_env, range(len(data)))\n",
    "# pool.close()\n",
    "# # envs_arr = np.array(all_envs)\n",
    "# # envs_df = pd.DataFrame(data=envs_arr, index = envs_arr[:, 0], columns = ['CATAID']+[str(i) for i in try_distances])\n",
    "\n",
    "# # df = pd.merge(envs_df, zdf, on='CATAID')\n",
    "# # df.to_csv('enviros.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no clue what to plot here. . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the precomputed 2PCF\n",
    "\n",
    "Download the 2PCF at several redshifts [here](https://drive.google.com/drive/folders/1eGlAO_wl9h0xiXiTMKV_m7h9YCRhDHP_?usp=sharing).\n",
    "\n",
    "Note that the data is $\\Delta^{2}(k)$, not the more familiar (to me) $\\mathcal{P}(k)$.  (A reminder of the relationship between them can be found [here](http://universe-review.ca/R05-04-powerspectrum.htm), particularly in [this figure](http://universe-review.ca/I02-20-correlate1b.png).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pk = np.genfromtxt('NptFns/0.042ngpps_new.dat_LOS1').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(pk[0], pk[1])\n",
    "# plt.semilogx()\n",
    "# plt.semilogy()\n",
    "# plt.xlabel(r'$k$ [Mpc/h]')\n",
    "# plt.ylabel(r'$\\Delta^2(k)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmin = 2 * np.pi / max(pk[0])\n",
    "# rmax = 2 * np.pi / min(pk[0])\n",
    "# print((rmin, rmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Ultimately, we will need to calculate the 2 and 3+ point correlation functions of the particle data.\n",
    "Because the data is split into 64 files per redshift, we also need a way to combine the positional information from each file to get coherent correlation functions.\n",
    "We may be able to more easily accomplish both goals if we first smooth the data using a Fourier-space basis like wavelets.\n",
    "\n",
    "## combine particle data from multiple files\n",
    "\n",
    "## calculate the N-point correlation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recidivator (Python 3)",
   "language": "python",
   "name": "recidivator_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
