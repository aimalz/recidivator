{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the __SLICS-HR__ particle data\n",
    "notebook by _Alex Malz (GCCL@RUB)_, (add your name here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy as ap\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're only considering the first 8 SLICS snapshots because the GAMA data doesn't have good enough coverage beyond that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_SLICS = np.array([0.042, 0.080, 0.130, 0.221, 0.317, 0.418, 0.525, 0.640])\n",
    "#, 0.764, 0.897, 1.041, 1.199, 1.372, 1.562, 1.772, 2.007, 2.269, 2.565, 2.899])\n",
    "z_mids = (z_SLICS[1:] + z_SLICS[:-1]) / 2.\n",
    "z_bins = np.insert(z_mids, 0, 0.023)\n",
    "z_bins = np.append(z_bins, 3.066)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "\n",
    "Download one of the 64 nodes $\\times$ 20 redshifts files at each redshift from Joachim Harnois-Deraps to start.\n",
    "I chose file 21 at $z=0.042$ for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_z = 2\n",
    "z_str = '{:<05}'.format(str(z_SLICS[which_z]))\n",
    "fn_base = 'xv'\n",
    "fn_index = 21\n",
    "fn_ext = '.dat'\n",
    "fn = z_str + fn_base + str(fn_index) + fn_ext\n",
    "data_dir = 'particle_data/cuillin.roe.ac.uk/~jharno/SLICS/SLICS_HR/LOS1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in from binary float(4) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_each = 'f' + str(4)\n",
    "dt = np.dtype([('x', dt_each), ('y', dt_each), ('z', dt_each), ('vx', dt_each), ('vy', dt_each), ('vz', dt_each)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, fn), 'rb') as f1:\n",
    "    raw_data = np.fromfile(f1, dtype=dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throw out first 12 entries as unwanted header information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data = pd.DataFrame(data=raw_data[2:], columns=['x', 'y', 'z', 'vx', 'vy', 'vz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(loc_data['x'], loc_data['y'], bins=(200, 200), norm=mpl.colors.LogNorm(), cmap='Spectral_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_to_plot = loc_data.sample(50000)\n",
    "# plt.scatter(loc_to_plot['x'], loc_to_plot['y'], marker='.', s=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to physical units\n",
    "\n",
    "The particle data starts out in simulation units relative to the per-node subvolume and needs to be converted to physical units in the space of all subvolumes before the whole volume can be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of MPI tasks per dimension\n",
    "nodes_dim = 4\n",
    "\n",
    "# volume size\n",
    "rnc = 3072.\n",
    "\n",
    "# subvolume size\n",
    "ncc = rnc / nodes_dim\n",
    "\n",
    "# physical scale in Mpc/h\n",
    "phys_scale = 505."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the conversion below makes sense for `x`, `y`, and `z` but not for `vx`, `vy`, and `vz`.\n",
    "Because of how the data is distributed across the files, I think 21, 22, 25, 26, 37, 38, 41, 42 are \"adjacent\" and free of edge effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes_coords = np.empty((nodes_dim, nodes_dim, nodes_dim))\n",
    "for k1 in range(1, nodes_dim+1):\n",
    "    for j1 in range(1, nodes_dim+1):\n",
    "        for i1 in range(1, nodes_dim+1):\n",
    "            current_ind = (i1 - 1) + (j1 - 1) * nodes_dim + (k1 - 1) * nodes_dim ** 2\n",
    "            node_coords = {'x': i1 - 1, 'y': j1 - 1, 'z': k1 - 1}\n",
    "            if fn_index == current_ind:\n",
    "                print('found index '+str(fn_index)+' at '+str((i1, j1, k1)))\n",
    "                true_node_coords = node_coords\n",
    "            all_nodes_coords[node_coords['x'], node_coords['y'], node_coords['z']] = current_ind\n",
    "            \n",
    "# print(all_nodes_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get coherent coordinates across all files, we need to shift them accordingly.\n",
    "The next cell is unexpectely slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift data\n",
    "glob_data = loc_data\n",
    "for col in ['x', 'y', 'z']:\n",
    "    glob_data[col] = np.remainder(loc_data[col] + true_node_coords[col] * ncc, rnc)\n",
    "    assert(max(glob_data[col] <= rnc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Mpc/h\n",
    "phys_data = glob_data * phys_scale / rnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in ['x', 'y', 'z']:\n",
    "    plt.hist(phys_data[dim], density=True, alpha=0.5)\n",
    "plt.xlabel('distance (Mpc/h)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(phys_data['x'], phys_data['y'], bins=(200,200), norm=mpl.colors.LogNorm(), cmap='Spectral_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_to_plot = phys_data.sample(50000)\n",
    "# plt.scatter(phys_to_plot['x'], phys_to_plot['y'], marker='.', s=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much data do we need?\n",
    "\n",
    "Obtain necessary depth from ~~[Ned Wright's cosmology calculator](http://www.astro.ucla.edu/~wright/CosmoCalc.html)~~ `astropy`.\n",
    "The SLICS cosmology has $\\Omega_{m} = 0.2905$, $\\Omega_{\\Lambda} = 0.7095$, $\\Omega_{b} = 0.0473$, $h = 0.6898$, $\\sigma_{8} = 0.826$, and $n_{s} = 0.969$.\n",
    "Let's assume the naive Cartesian-to-angular coordinates and flatten along the `z` direction.\n",
    "We need to flatten a depth corresponding to the bounds of each redshift bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.6898\n",
    "cosmo = FlatLambdaCDM(H0=100.*h, Om0=0.2905, Ob0=0.0473)\n",
    "d_comov = []\n",
    "for z in z_bins:\n",
    "    dc = cosmo.comoving_distance(float(z))\n",
    "    d_comov.append(dc.value / h)\n",
    "d_comov = np.array(d_comov)\n",
    "depths = d_comov[1:] - d_comov[:-1]\n",
    "\n",
    "avg_d_comov = []\n",
    "for z in z_SLICS:\n",
    "    dc = cosmo.comoving_distance(float(z))\n",
    "    avg_d_comov.append(dc.value / h)\n",
    "    \n",
    "# print(depths)\n",
    "# print(avg_d_comov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, `depths` < `phys_scale` $Mpc/h$ only in the first three redshift bins, meaning the depths of the next five GAMA redshift bin may require opening two files.\n",
    "I think the way they're arranged means that (21, 37), (22, 38), (25, 41), and (26, 42) are pairs adjacent in `z`.\n",
    "\n",
    "_This is as good a time as any to note that our mock catalog will have a bit of a degeneracy if we use the same file numbers for all redshifts because each file corresponds to the same physical volume across cosmic time, whereas in a real survey, our redshift bins contain different volumes/galaxies.\n",
    "We have a choice to make about discontinuities or non-physical repetitition._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to angular units\n",
    "\n",
    "Obtain angular diameter distance $d_{a}$ in units $\\theta = x / d_{a}$ with $d_{a} = d_{c} / (1 + z)$, where $d_{c}$ is the comoving diameter distance and $x$ is the distance in the SLICS data.\n",
    "Compare with the GAMA footprint of $286^{\\circ^{2}} * (\\pi / 180^{\\circ})^{2} \\approx 0.087 sr$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ang = avg_d_comov / (1 + z_SLICS)\n",
    "theta_box = phys_scale / d_ang * 180. / np.pi\n",
    "footprint = theta_box**2\n",
    "# print(footprint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling behavior is as expected;\n",
    "`phys_scale` subtends a larger angle at low redshifts and a smaller angle at high redshifts.\n",
    "One file's worth of SLICS data subtends an angular area larger than the GAMA footprint in the first five GAMA redshift bins, but the next three GAMA redshift bins would definitely require more than one file's worth of data.\n",
    "We need to pick an angular area for our mock galaxy catalog.\n",
    "Let's go with twice that for now.\n",
    "_Do we think twice the GAMA area is sufficiently compelling?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_gama = 286.\n",
    "GAMA_phys_scale = np.sqrt(theta_gama) * (np.pi / 180.) * d_ang\n",
    "# print(2. * GAMA_phys_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go with twice the GAMA footprint, then the first three redshift bins need only one file, the next three need two, and the last two need three.\n",
    "I think (21, 22), (25, 26), (37, 38), and (41, 42) are adjacent in `x`/`RA` and (21, 25), (22, 26), (37, 41), and (38, 42) are adjacent in `y`/`DEC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     j = i+1\n",
    "#     subset = phys_data[(phys_data['x'] <= 10.*j) & (phys_data['y'] <= 10.*j) & (phys_data['z'] <= 10.*j)]\n",
    "#     subset.to_csv('spat'+str(j)+'0Mpc.csv', header=False, index=False, sep=' ', columns=['x', 'y', 'z'])\n",
    "#     angular = subset / 313.5 * 69.6 / 100. * float(j) * 180 / np.pi\n",
    "#     print((min(angular['x']), max(angular['x'])))\n",
    "#     print((min(angular['y']), max(angular['y'])))  \n",
    "#     angular.to_csv('ang'+str(j)+'deg.csv', header=False, index=False, sep=',', columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, I'm not going to deal with combining adjacent files, just chopping up ones that are too big.\n",
    "This is slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ang_data = phys_data[np.mod(phys_data['z'] - min(phys_data['z']), phys_scale) < depths[which_z]]\n",
    "ang_data['RA'] = ang_data['x'] / d_ang[1] * 180. / np.pi\n",
    "ang_data['DEC'] = ang_data['y'] / d_ang[1] * 180. / np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd change this for the area of our mock survey when we decide on it.\n",
    "_There is an edge effect going on right now.\n",
    "I need to switch to one of the internal files to avoid roll-over that's breaking min/max checks._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_theta = np.sqrt(2. * theta_gama)\n",
    "cut_data = ang_data[(ang_data['RA'] < lim_theta + min(ang_data['RA']))\n",
    "                    & (ang_data['DEC'] < lim_theta + min(ang_data['DEC']))]\n",
    "\n",
    "# plt.hist(cut_data['RA'])\n",
    "# plt.hist(cut_data['DEC'])\n",
    "\n",
    "cut_data.to_csv(z_str+'cut.csv', header=True, index=False, sep=',', columns=['RA', 'DEC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(cut_data['RA'], cut_data['DEC'], bins=(200,200), norm=mpl.colors.LogNorm(), cmap='Spectral_r')\n",
    "plt.xlabel('RA (deg)')\n",
    "plt.ylabel('DEC (deg)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_to_plot = cut_data.sample(50000)\n",
    "# plt.scatter(cut_to_plot['x'], cut_to_plot['y'], marker='.', s=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch after here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatially subsample data\n",
    "\n",
    "Turns out 1/64th of the total data was still way more than we could reasonably use at once to compute correlation functions!\n",
    "This should really be sliced by size of box.\n",
    "First, just break it up into smaller boxes.\n",
    "Let's say we want $10^{5}$ particles per box, so we'll cut it in 16 in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # distances = np.sqrt(phys_data['x']**2 + phys_data['y']**2 + phys_data['z']**2)\n",
    "# splitpoints = {}\n",
    "# for dim in ['RA', 'DEC']:\n",
    "#     splitpoints[dim] = np.linspace(min(ang_data[dim]), max(ang_data[dim]), 17)\n",
    "#     print(splitpoints[dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(16):\n",
    "#     for j in range(16):\n",
    "#         subsample = ang_data.loc[(ang_data['RA'] >= splitpoints['RA'][i]) & (ang_data['RA'] <= splitpoints['RA'][i+1])\\\n",
    "#                                  & (ang_data['DEC'] >= splitpoints['DEC'][j]) & (ang_data['DEC'] <= splitpoints['DEC'][j+1])]\n",
    "#         subsample.to_csv(z_str+'slice_'+str(i)+'_'+str(j)+'.csv', header=True, index=False, sep=',', columns=['RA', 'DEC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly subsample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(angular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_plot = angular.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist2d(to_plot['x'], to_plot['y'], bins=100, norm=mpl.colors.LogNorm(), cmap='Spectral_r')\n",
    "# plt.savefig('mock_gal_pos.png', dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_distances = np.flip(np.geomspace(0.01, 1.0, 10), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import environment as galenv\n",
    "\n",
    "# def calc_env(ind):\n",
    "#     res = []\n",
    "#     friends = data\n",
    "#     for dist in try_distances:\n",
    "#         friends = galenv.nn_finder(friends, data[ind], dist)\n",
    "#         res.append(len(friends))\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [to_plot['x'].values, to_plot['y'].values]\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.array([to_plot['x'].values, to_plot['y'].values]).T\n",
    "# nps = mp.cpu_count()\n",
    "# pool = mp.Pool(nps - 1)\n",
    "# envs = pool.map(calc_env, range(len(data)))\n",
    "# pool.close()\n",
    "# # envs_arr = np.array(all_envs)\n",
    "# # envs_df = pd.DataFrame(data=envs_arr, index = envs_arr[:, 0], columns = ['CATAID']+[str(i) for i in try_distances])\n",
    "\n",
    "# # df = pd.merge(envs_df, zdf, on='CATAID')\n",
    "# # df.to_csv('enviros.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no clue what to plot here. . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the precomputed 2PCF\n",
    "\n",
    "Download the 2PCF at several redshifts [here](https://drive.google.com/drive/folders/1eGlAO_wl9h0xiXiTMKV_m7h9YCRhDHP_?usp=sharing).\n",
    "\n",
    "Note that the data is $\\Delta^{2}(k)$, not the more familiar (to me) $\\mathcal{P}(k)$.  (A reminder of the relationship between them can be found [here](http://universe-review.ca/R05-04-powerspectrum.htm), particularly in [this figure](http://universe-review.ca/I02-20-correlate1b.png).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pk = np.genfromtxt('NptFns/0.042ngpps_new.dat_LOS1').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(pk[0], pk[1])\n",
    "# plt.semilogx()\n",
    "# plt.semilogy()\n",
    "# plt.xlabel(r'$k$ [Mpc/h]')\n",
    "# plt.ylabel(r'$\\Delta^2(k)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmin = 2 * np.pi / max(pk[0])\n",
    "# rmax = 2 * np.pi / min(pk[0])\n",
    "# print((rmin, rmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Ultimately, we will need to calculate the 2 and 3+ point correlation functions of the particle data.\n",
    "Because the data is split into 64 files per redshift, we also need a way to combine the positional information from each file to get coherent correlation functions.\n",
    "We may be able to more easily accomplish both goals if we first smooth the data using a Fourier-space basis like wavelets.\n",
    "\n",
    "## combine particle data from multiple files\n",
    "\n",
    "## calculate the N-point correlation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recidivator (Python 3)",
   "language": "python",
   "name": "recidivator_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
