{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the __SLICS-HR__ particle data\n",
    "notebook by _Alex Malz (GCCL@RUB)_, (add your name here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading in data\n",
    "\n",
    "Download one of the 64 files at each redshift from [here](http://cuillin.roe.ac.uk/~jharno/SLICS/SLICS_HR/LOS1/) to start.\n",
    "I chose file 27 at $z=0.042$ for this example.\n",
    "Read in from binary float(4) format and throw out first 12 entries as unwanted header information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_str = '0.042'\n",
    "fn_base = 'xv'\n",
    "fn_index = 27\n",
    "fn_ext = '.dat'\n",
    "fn = z_str + fn_base + str(fn_index) + fn_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_each = 'f'+str(4)\n",
    "dt = np.dtype([('x', dt_each), ('y', dt_each), ('z', dt_each), ('vx', dt_each), ('vy', dt_each), ('vz', dt_each)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn, 'rb') as f1:\n",
    "    raw_data = np.fromfile(f1, dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data = pd.DataFrame(data=raw_data[2:], columns=['x', 'y', 'z', 'vx', 'vy', 'vz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coordinate transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of MPI tasks per dimension\n",
    "nodes_dim = 4\n",
    "\n",
    "# subvolume size\n",
    "ncc = 768\n",
    "\n",
    "# volume size\n",
    "rnc = 3072."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k1 in range(1, nodes_dim+1):\n",
    "    for j1 in range(1, nodes_dim+1):\n",
    "        for i1 in range(1, nodes_dim+1):\n",
    "            if fn_index == (i1 - 1) + (j1 - 1) * nodes_dim + (k1 - 1) * nodes_dim ** 2:\n",
    "                print('found index '+str(fn_index)+' at '+str((i1, j1, k1)))\n",
    "                node_coords = {'x': i1 - 1, 'y': j1 - 1, 'z': k1 - 1}\n",
    "                print(node_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift data\n",
    "glob_data = loc_data\n",
    "for col in ['x', 'y', 'z']:\n",
    "    glob_data[col] = np.remainder(loc_data[col] + node_coords[col] * ncc, rnc)\n",
    "    assert(max(glob_data[col] <= rnc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Mpc/h\n",
    "phys_data = glob_data * 505. / 3072."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Ultimately, we will need to calculate the 2 and 3+ point correlation functions of the particle data.\n",
    "Because the data is split into 64 files per redshift, we also need a way to combine the positional information from each file to get coherent correlation functions.\n",
    "We may be able to more easily accomplish both goals if we first smooth the data using a Fourier-space basis like wavelets.\n",
    "\n",
    "## combine particle data from multiple files\n",
    "\n",
    "## calculate the N-point correlation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COIN (Python 3)",
   "language": "python",
   "name": "coin_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
