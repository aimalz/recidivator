{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the __SLICS-HR__ particle data\n",
    "notebook by _Alex Malz (GCCL@RUB)_, (add your name here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "\n",
    "Download one of the 64 nodes $\\times$ 20 redshifts files at each redshift from Joachim Harnois-Deraps to start.\n",
    "I chose file 0 at $z=0.080$ for this example.\n",
    "Read in from binary float(4) format and throw out first 12 entries as unwanted header information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_str = '0.080'\n",
    "fn_base = 'xv'\n",
    "fn_index = 0\n",
    "fn_ext = '.dat'\n",
    "fn = z_str + fn_base + str(fn_index) + fn_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_each = 'f' + str(4)\n",
    "dt = np.dtype([('x', dt_each), ('y', dt_each), ('z', dt_each), ('vx', dt_each), ('vy', dt_each), ('vz', dt_each)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn, 'rb') as f1:\n",
    "    raw_data = np.fromfile(f1, dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data = pd.DataFrame(data=raw_data[2:], columns=['x', 'y', 'z', 'vx', 'vy', 'vz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to physical units\n",
    "\n",
    "The particle data starts out in simulation units relative to the per-node subvolume and needs to be converted to physical units in the space of all subvolumes before the whole volume can be considered.\n",
    "Note that the conversion below makes sense for `x`, `y`, and `z` but not for `vx`, `vy`, and `vz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of MPI tasks per dimension\n",
    "nodes_dim = 4\n",
    "\n",
    "# subvolume size\n",
    "ncc = 768\n",
    "\n",
    "# volume size\n",
    "rnc = 3072."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k1 in range(1, nodes_dim+1):\n",
    "    for j1 in range(1, nodes_dim+1):\n",
    "        for i1 in range(1, nodes_dim+1):\n",
    "            if fn_index == (i1 - 1) + (j1 - 1) * nodes_dim + (k1 - 1) * nodes_dim ** 2:\n",
    "                print('found index '+str(fn_index)+' at '+str((i1, j1, k1)))\n",
    "                node_coords = {'x': i1 - 1, 'y': j1 - 1, 'z': k1 - 1}\n",
    "                print(node_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift data\n",
    "glob_data = loc_data\n",
    "for col in ['x', 'y', 'z']:\n",
    "    glob_data[col] = np.remainder(loc_data[col] + node_coords[col] * ncc, rnc)\n",
    "    assert(max(glob_data[col] <= rnc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Mpc/h\n",
    "phys_data = glob_data * 505. / 3072."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(phys_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(phys_data['x'], alpha=0.25, density=True)\n",
    "plt.hist(phys_data['y'], alpha=0.25, density=True)\n",
    "plt.hist(phys_data['z'], alpha=0.25, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatially subsample data\n",
    "\n",
    "Turns out 1/64th of the total data was still way more than we could reasonably use at once to compute correlation functions!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the box by scale not number of galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.sqrt(phys_data['x']**2 + phys_data['y']**2 + phys_data['z']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_data.iloc[order[:100]].to_csv('pos100.csv', header=False, index=False, sep=' ', columns=['x', 'y', 'z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_data.iloc[order[:1000]].to_csv('pos1000.csv', header=False, index=False, sep=' ', columns=['x', 'y', 'z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_data.iloc[order[:10000]].to_csv('pos10000.csv', header=False, index=False, sep=' ', columns=['x', 'y', 'z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_data.iloc[order[:100000]].to_csv('pos100000.csv', header=False, index=False, sep=' ', columns=['x', 'y', 'z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_data.iloc[order[:1000000]].to_csv('pos1000000.csv', header=False, index=False, sep=' ', columns=['x', 'y', 'z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the precomputed 2PCF\n",
    "\n",
    "Download the 2PCF at several redshifts [here](https://drive.google.com/drive/folders/1eGlAO_wl9h0xiXiTMKV_m7h9YCRhDHP_?usp=sharing).\n",
    "\n",
    "Note that the data is $\\Delta^{2}(k)$, not the more familiar (to me) $\\mathcal{P}(k)$.  (A reminder of the relationship between them can be found [here](http://universe-review.ca/R05-04-powerspectrum.htm), particularly in [this figure](http://universe-review.ca/I02-20-correlate1b.png).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk = np.genfromtxt('NptFns/0.042ngpps_new.dat_LOS1').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pk[0], pk[1])\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.xlabel(r'$k$ [Mpc/h]')\n",
    "plt.ylabel(r'$\\Delta^2(k)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmin = 2 * np.pi / max(pk[0])\n",
    "rmax = 2 * np.pi / min(pk[0])\n",
    "print((rmin, rmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Ultimately, we will need to calculate the 2 and 3+ point correlation functions of the particle data.\n",
    "Because the data is split into 64 files per redshift, we also need a way to combine the positional information from each file to get coherent correlation functions.\n",
    "We may be able to more easily accomplish both goals if we first smooth the data using a Fourier-space basis like wavelets.\n",
    "\n",
    "## combine particle data from multiple files\n",
    "\n",
    "## calculate the N-point correlation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COIN (Python 3)",
   "language": "python",
   "name": "coin_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
