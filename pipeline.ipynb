{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `recidivator` pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing the SLICS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy as ap\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "# import dask\n",
    "# import dask.dataframe as dd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from scipy.stats import gaussian_kde\n",
    "import scipy.optimize as spo\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_SLICS = np.array([0.042, 0.080, 0.130, 0.221, 0.317, 0.418, 0.525])\n",
    "#, 0.640, 0.764, 0.897, 1.041, 1.199, 1.372, 1.562, 1.772, 2.007, 2.269, 2.565, 2.899])\n",
    "\n",
    "h = 0.6898\n",
    "cosmo = FlatLambdaCDM(H0=100.*h, Om0=0.2905, Ob0=0.0473)\n",
    "\n",
    "N_part = 1536**3\n",
    "M_part = 2.88e9 * ap.units.M_sun * h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_each = 'f' + str(4)\n",
    "dt = np.dtype([('x', dt_each), ('y', dt_each), ('z', dt_each), ('vx', dt_each), ('vy', dt_each), ('vz', dt_each)])\n",
    "\n",
    "# number of MPI tasks per dimension\n",
    "nodes_dim = 4\n",
    "\n",
    "# volume size\n",
    "rnc = 3072.\n",
    "\n",
    "# subvolume size\n",
    "ncc = rnc / nodes_dim\n",
    "\n",
    "# physical scale in Mpc/h\n",
    "phys_scale = 505."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: specific to using one SLICS file at a time rather than combining!\n",
    "N_file = N_part / (nodes_dim**3)\n",
    "N_slice = N_file\n",
    "\n",
    "# whole slics comoving size\n",
    "slics_comov = phys_scale * h * ap.units.Mpc\n",
    "fileside_comov = slics_comov / float(nodes_dim)\n",
    "# whole slics proper size at each redshift\n",
    "slics_prop = slics_comov / (1 + z_SLICS)\n",
    "fileside_prop = slics_prop / float(nodes_dim)\n",
    "\n",
    "# box depth in comoving coordinates\n",
    "boxdepth = fileside_comov#phys_scale / float(nodes_dim) * h * 1e6 * ap.units.pc#comoving distance in pc\n",
    "# box side length in physical coordinates as a function of redshift\n",
    "boxside = fileside_prop#proper distance in pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comoving distance to SLICS snapshot\n",
    "d_comov = []\n",
    "for z in z_SLICS:\n",
    "    dc = cosmo.comoving_distance(float(z))\n",
    "    d_comov.append(dc.value)\n",
    "d_comov = np.array(d_comov) * ap.units.Mpc\n",
    "\n",
    "# redshift bin ends around SLICS snapshot\n",
    "for j in range(nodes_dim):\n",
    "    i = j + 1\n",
    "    d_comov_mins = d_comov - i * boxdepth / 2.\n",
    "    d_comov_maxs = d_comov + i * boxdepth / 2.\n",
    "    min_zs = np.array([ap.cosmology.z_at_value(cosmo.comoving_distance, d_comov_min) for d_comov_min in d_comov_mins])\n",
    "    max_zs = np.array([ap.cosmology.z_at_value(cosmo.comoving_distance, d_comov_max) for d_comov_max in d_comov_maxs])\n",
    "\n",
    "    with open('DEAR/Data/'+str(i)+'file-z-ends.txt', 'wb') as zbinfile:\n",
    "        zbinfile.write(b'# z_SLICS min_z max_z\\n')\n",
    "        np.savetxt(zbinfile, np.vstack((z_SLICS, min_zs, max_zs)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_ang = 2.5#5.\n",
    "\n",
    "# angular diameter distance to SLICS snapshot\n",
    "d_ang = (d_comov_maxs + d_comov_mins) / 2. / (1 + z_SLICS)\n",
    "# angular diameter on the sky of one SLICS file from each redshift snapshot\n",
    "angbox = (boxside / d_ang) * (180. / np.pi * ap.units.degree)\n",
    "# fraction of a SLICS file's length (in any linear coordinates) that will subtend 2.5 degrees on sky\n",
    "side_frac = crit_ang * ap.units.degree / angbox\n",
    "\n",
    "#NOTE: change this assertion once there's infrastructure for joining SLICS files in the plane\n",
    "assert(np.all(side_frac <= 1.))\n",
    "\n",
    "\n",
    "avg_dens_SLICS = N_part / ((angbox * nodes_dim) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble_combos = {}\n",
    "for i in range(len(z_SLICS)):\n",
    "    for j in [41]:#21, 22, 25, 26, 37, 38, 41, 42]:\n",
    "        #NOTE: j loop will change a lot when SLICS files are combined\n",
    "        bubble_combos['z'+str(i)+'box'+str(j)] = (i, j)\n",
    "        \n",
    "pathname = 'DEAR/Data/const_ang'\n",
    "if os.path.isdir(pathname) == False:\n",
    "    os.system('mkdir '+ pathname)\n",
    "for testz in range(len(z_SLICS)):\n",
    "    zpath = pathname+'/z'+str(testz)\n",
    "    if os.path.isdir(zpath) == False:\n",
    "        os.system('mkdir '+ zpath)\n",
    "\n",
    "nps = 20#mp.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline functions: make manageable bubbles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: combine z coordinates over multiple files, using bash scripts.\n",
    "Because of how the data is distributed across the files, I think 21, 22, 25, 26, 37, 38, 41, 42 are \"adjacent\" and free of edge effects.\n",
    "_Note_: We can just have this be an automated check, knowing that files are adjacent when their `node_coords` are the same aside from being off by one in one of their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try dask for speed!\n",
    "\n",
    "def help_read(which_z, fn_index):\n",
    "    z_str = '{:<05}'.format(str(z_SLICS[which_z]))\n",
    "    fn_base = 'xv'\n",
    "    fn_ext = '.dat'\n",
    "    fn = z_str + fn_base + str(fn_index) + fn_ext\n",
    "    data_dir = 'SLICS/particle_data/cuillin.roe.ac.uk/~jharno/SLICS/SLICS_HR/LOS1'\n",
    "    with open(os.path.join(data_dir, fn), 'rb') as f1:\n",
    "        raw_data = np.fromfile(f1, dtype=dt)\n",
    "    loc_data = pd.DataFrame(data=raw_data[2:], columns=['x', 'y', 'z', 'vx', 'vy', 'vz'])\n",
    "    if loc_data.duplicated().any():\n",
    "        print('duplicates found in z='+str(z_SLICS[which_z])+' box='+str(fn_index)+'!')\n",
    "    loc_data.drop_duplicates()\n",
    "    assert(~loc_data.duplicated().any())\n",
    "    return(loc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: finedensity to pick sane bubble centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_one_bubble(one_key):\n",
    "    (testz, testfn) = bubble_combos[one_key]\n",
    "    print('starting z='+str(z_SLICS[testz]))\n",
    "    zpath = pathname+'/z'+str(testz)\n",
    "    boxpath = zpath+'/box'+str(testfn)\n",
    "    if os.path.isdir(boxpath) == False:\n",
    "        os.system('mkdir '+ boxpath)\n",
    "    elif os.listdir(boxpath) != []:\n",
    "        print('not-rerunning z='+str(z_SLICS[testz])+' box='+str(testfn))\n",
    "        return None\n",
    "    print('starting box='+str(testfn)+', loading SLICS data (the slow step)')\n",
    "    loc_data = help_read(which_z=testz, fn_index=testfn)\n",
    "    print('loaded SLICS data, identifying bubble centers, moving on')\n",
    "    extremex = np.array([loc_data['x'].min(), loc_data['x'].max()])\n",
    "    extremey = np.array([loc_data['y'].min(), loc_data['y'].max()])\n",
    "#     print((extremex, extremey))\n",
    "#     (coarsedensity, xedges, yedges) = np.histogram2d(loc_data['x'], loc_data['y'], bins=int(resolution))\n",
    "    center = np.array([extremex[0] + extremex[1], extremey[0] + extremey[1]]) / 2.\n",
    "    radius = np.array([extremex[1] - extremex[0], extremey[1] - extremey[0]]) * side_frac[testz] / 2.\n",
    "    bubble = loc_data.loc[lambda df: (df['x'] > center[0] - radius[0]) & (df['x'] < center[0] + radius[0])\n",
    "                          & (df['y'] > center[1] - radius[1]) & (df['y'] < center[1] + radius[1]), :]\n",
    "#     pkl.dump((coarsedensity, xedges, yedges), boxpath+'/coarsedensity.p')\n",
    "#     extreme = np.quantile(coarsedensity.flatten(), 0.99)\n",
    "#     indcenters = np.argwhere(coarsedensity > extreme)\n",
    "#     #NOTE: finedensity step would go here\n",
    "#     xcenters = (xedges[indcenters.T[0]] + xedges[indcenters.T[0]+1]) / 2\n",
    "#     ycenters = (yedges[indcenters.T[1]] + yedges[indcenters.T[1]+1]) / 2\n",
    "# #     print('identified bubble centers, going through each bubble')\n",
    "# #     bubbles, globs, pos_mpc, pos_ang = [], [], [], []\n",
    "#     for i, center in enumerate(indcenters):\n",
    "#         bubpath = boxpath+'/bub'+str(i)\n",
    "#         if os.path.isdir(bubpath) == False:\n",
    "#             os.system('mkdir '+ bubpath)\n",
    "#         bubble = loc_data.loc[lambda df: (df['x'] > xcenters[i] - rmin) & (df['x'] < xcenters[i] + rmin)\n",
    "#                           & (df['y'] > ycenters[i] - rmin) & (df['y'] < ycenters[i] + rmin), :]\n",
    "# #         if ~bubble.duplicated().any():\n",
    "# #             print('no duplicate coordinates in x, y, z, vx, vy, vz')\n",
    "# #         plt.hist2d(bubble['x'], bubble['y'], bins=(200, 200), norm=mpl.colors.LogNorm(), cmap='Spectral_r')\n",
    "# #         plt.savefig(bubpath+'/bubble_raw.png')\n",
    "#         bubble.to_csv(bubpath+'/particles.csv', index=False)\n",
    "    bubble.to_csv(boxpath+'/particles.csv', index=False)\n",
    "# #         print('saved bubble particles to not have to load whole SLICS file again, next transform data')\n",
    "# #         bubbles.append(bubble)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pool = mp.Pool(nps)\n",
    "# pool.map(isolate_one_bubble, bubble_combos.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline functions: transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_find_coords(fn_index):\n",
    "#     all_nodes_coords = np.empty((nodes_dim, nodes_dim, nodes_dim))\n",
    "    for k1 in range(1, nodes_dim+1):\n",
    "        for j1 in range(1, nodes_dim+1):\n",
    "            for i1 in range(1, nodes_dim+1):\n",
    "                current_ind = (i1 - 1) + (j1 - 1) * nodes_dim + (k1 - 1) * nodes_dim ** 2\n",
    "                node_coords = {'x': i1 - 1, 'y': j1 - 1, 'z': k1 - 1}\n",
    "                if fn_index == current_ind:\n",
    "#                     print('found index '+str(fn_index)+' at '+str((i1, j1, k1)))\n",
    "                    true_node_coords = node_coords\n",
    "#                 all_nodes_coords[node_coords['x'], node_coords['y'], node_coords['z']] = current_ind\n",
    "                    return(true_node_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_shift(true_node_coords, loc_data):\n",
    "    # shift data\n",
    "    glob_data = loc_data\n",
    "    print(glob_data.columns)\n",
    "    for col in ['x', 'y']:#, 'z']:\n",
    "        glob_data[col] = np.remainder(loc_data[col] + true_node_coords[col] * ncc, rnc)\n",
    "        assert(max(glob_data[col] <= rnc))\n",
    "    return(glob_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_convert(glob_data):\n",
    "    # convert to Mpc\n",
    "    phys_data = glob_data / rnc * slics_comov\n",
    "    return(phys_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_one_bubble(one_key):\n",
    "    (testz, testfn) = bubble_combos[one_key]\n",
    "    true_node_coords = help_find_coords(fn_index=testfn)\n",
    "    zpath = pathname+'/z'+str(testz)\n",
    "    fullpath = zpath+'/box'+str(testfn)\n",
    "#     bubpaths = os.listdir(boxpath)\n",
    "#     fullpaths = [boxpath+'/'+bubpath+'/' for bubpath in bubpaths]\n",
    "#     for fullpath in fullpaths:\n",
    "    bubble = pd.read_csv(os.path.join(fullpath, 'particles.csv'))\n",
    "    glob_data = help_shift(true_node_coords, bubble)\n",
    "#         globs.append(glob_data)\n",
    "    phys_data = help_convert(glob_data)\n",
    "#         pos_mpc.append(phys_data)\n",
    "    ang_data = pd.DataFrame()\n",
    "    ang_data['RA'] = phys_data['x'] / d_ang[testz] * 180. / np.pi\n",
    "    ang_data['DEC'] = phys_data['y'] / d_ang[testz] * 180. / np.pi\n",
    "    plt.hist2d(ang_data['RA'], ang_data['DEC'], bins=(256, 256), norm=mpl.colors.LogNorm(), cmap='Spectral_r')\n",
    "    plt.savefig(os.path.join(fullpath, 'projection.png'))\n",
    "    if bubble.duplicated().any():\n",
    "        print('duplicate particles introduced by dropping z, vx, vy, vz')\n",
    "#         if bubble.duplicated().any():\n",
    "#             print('dropped duplicates')\n",
    "#             to_save = ang_data.drop_duplicates()\n",
    "#         pos_ang.append(ang_data.drop_duplicates())\n",
    "        #\n",
    "    ang_data.to_csv(os.path.join(fullpath, 'projection.csv'), index=False)\n",
    "    print('shifted from machine, converted to physical, projected to angular coordinates, and saved '+str(len(ang_data)))\n",
    "    return (one_key, len(ang_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pool = mp.Pool(nps)\n",
    "# npart_all = pool.map(project_one_bubble, bubble_combos.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model the large-scale structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import csv\n",
    "import matplotlib.pylab as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.interpolate as spi\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import R\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import rpy2.robjects as ro\n",
    "\n",
    "# # only run this once ever\n",
    "# utils = importr('utils')\n",
    "# utils.install_packages('spatstat')\n",
    "# utils.install_packa.groupby('a').count()ges('sparr')\n",
    "# utils.install_packages('dplyr')\n",
    "# utils.install_packages('magrittr')\n",
    "# utils.install_packages('reshape2')\n",
    "\n",
    "# import R packages\n",
    "\n",
    "ro.r('library(spatstat)')\n",
    "ro.r('library(sparr)')\n",
    "ro.r('library(dplyr)')\n",
    "ro.r('library(magrittr)')\n",
    "ro.r('library(reshape2)')\n",
    "\n",
    "# prevent truncation of floats from files\n",
    "ro.r('options(digits=20)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname = 'DEAR/Data/const_ang'\n",
    "\n",
    "bubble_combos = {}\n",
    "for i in range(len(z_SLICS)):\n",
    "    #Note: j loop will change a lot when SLICS files are combined\n",
    "    for j in [38]:#21, 22, 25, 26, 37, 38, 41, 42]:\n",
    "        boxpath = pathname+'/z'+str(i)+'/box'+str(j)\n",
    "#         nbub = len(os.listdir(boxpath))\n",
    "#         for k in range(nbub):\n",
    "#             for l in range(4):\n",
    "        bubble_combos[boxpath] = (i, j)#+'/patch'+str(k)] = (i, j, l, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 256# should be 1024\n",
    "bw_ival = 2.5 / (side_frac * boxside / 1.e6 / ap.units.pc) / np.log2(res)\n",
    "\n",
    "to_save = {'$z$v': 'kdegrid',\n",
    "           '$him$v': 'bwgrid',\n",
    "           '$z$xcol': 'xgrid',\n",
    "           '$z$yrow': 'ygrid',\n",
    "           '$h': 'bwpart'\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit a KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_kde(one_key):\n",
    "#     (chosenz, chosenbox, chosenbubble, chosenpatch) = bubble_combos[one_key]\n",
    "    (i, j) = bubble_combos[one_key]\n",
    "    # here I am reading one bubble of data\n",
    "# ro.r('dataRAW <- read.csv(\"tinypart.csv\")')\n",
    "    ro.r('dataRAW <- read.csv(\"'+pathname+'/'+one_key+'/projection.csv'+'\")')\n",
    "#     ro.r('dataRAW <- fread(\"'+pathname+'/'+one_key+'/projection.csv'+'\")')\n",
    "    # will have to change this to projection.csv\n",
    "#     nparticles = ro.r('nrows(dataRAW)')\n",
    "    \n",
    "# try data.table's fread\n",
    "\n",
    "# get single columns from data frame\n",
    "    ro.r('x <- dataRAW$RA')\n",
    "    ro.r('y <- dataRAW$DEC')\n",
    "\n",
    "# put in the correct format\n",
    "    ro.r('myPointData <- ppp(x, y, xrange=range(x), yrange=range(y))')\n",
    "    \n",
    "# read variables into R section\n",
    "    ro.r('h0 = '+str(bw_ival[i]))\n",
    "    ro.r('resolution = '+str(res))#4096\n",
    "    \n",
    "    ti = time.time()\n",
    "    ro.r('ddest <- bivariate.density(myPointData, h0=h0, adapt=TRUE, resolution=resolution, parallelise='+str(int(nps / len(z_SLICS)))+')')\n",
    "# # consider use.ppp.methods = TRUE\n",
    "    # ro.r('ddest <- multiscale.density(myPointData, h0=h0)')\n",
    "    tf = time.time()\n",
    "    dt = tf - ti\n",
    "    message = 'z'+str(i)+'/box'+str(j)+' kde completed in '+str(dt)+' seconds\\n'\n",
    "    print(message)\n",
    "\n",
    "    # ro.r('write.csv(ddest$z$v, \"'+kdefn+'\")')\n",
    "    for key in to_save.keys():\n",
    "        ro.r('write.csv(ddest'+key+', \"'+pathname+'/z'+str(i)+'/box'+str(j)+'/'+to_save[key]+'.csv\")')\n",
    "    \n",
    "    with open(os.path.join(pathname, 'progress.txt'), 'a') as stdout:\n",
    "        stdout.write(message)\n",
    "        # write essential output to files\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pool = mp.Pool(nps)\n",
    "# pool.map(one_kde, bubble_combos.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample the KDEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NOTE: this should be expected value of a galaxy mass function\n",
    "avg_gal_mass = 1.#.e12 * ap.units.M_sun\n",
    "\n",
    "npatch = 4\n",
    "\n",
    "# gama_xlims, gama_ylims = {}, {}\n",
    "# gama_xlims[0] = [ 30.20075 , 38.79967 ]\n",
    "# gama_ylims[0] = [ -10.24806 , -3.34789 ]\n",
    "# gama_xlims[1] = [ 129.00008 , 140.99921 ]\n",
    "# gama_ylims[1] = [ -1.9999900000000002 , 2.99992 ]\n",
    "# gama_xlims[2] = [ 173.69011 , 185.99942 ]\n",
    "# gama_ylims[2] = [ -2.99973 , 2.00243 ]\n",
    "# gama_xlims[3] = [ 211.49796 , 223.49988 ]\n",
    "# gama_ylims[3] = [ -1.99995 , 2.9999599999999997 ]\n",
    "\n",
    "allz_gama_areas, allz_gama_dens, allz_gama_cts = [], [], []\n",
    "# allz_gama_coords = []\n",
    "for i in range(len(z_SLICS)):\n",
    "    gama_areas, gama_dens, gama_cts = np.empty(npatch), np.empty(npatch), np.empty(npatch)\n",
    "#     gama_coords = np.empty(4)\n",
    "    gama = pd.read_csv('../environmet_clustering/classes/z_'+'{:<05}'.format(str(z_SLICS[i]))+'_manygroups_oneslice.csv')\n",
    "    print(gama.columns)\n",
    "    for l in gama['patch'].unique():\n",
    "#         gama_coords[l] = gama[gama['patch'] == l][['RA_x', 'DEC_x']]\n",
    "#         what_is_bubble(gama_coords[l], 'RA_x', 'DEC_x', i)\n",
    "        gama_xlims = [gama[gama['patch'] == l]['RA_x'].min(), gama[gama['patch'] == l]['RA_x'].max()]\n",
    "        gama_ylims = [gama[gama['patch'] == l]['DEC_x'].min(), gama[gama['patch'] == l]['DEC_x'].max()]\n",
    "        gama_areas[l] = (gama_xlims[1] - gama_xlims[0]) * (gama_ylims[1] - gama_ylims[0])\n",
    "        gama_cts[l] = gama.groupby('patch').count()['CATAID'][l]\n",
    "        gama_dens[l] = avg_gal_mass * gama_cts[l] / gama_areas[l]\n",
    "    allz_gama_areas.append(gama_areas)\n",
    "    allz_gama_dens.append(gama_dens)\n",
    "    allz_gama_cts.append(gama_cts)\n",
    "#     allz_gama_coords.append(gama_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall definition of galaxy bias b_g = delta_g / delta_m = (rho_g - bar[rho_g]) / (rho_m - bar[rho_m]) * (bar[rho_m] / bar[rho_g])\n",
    "# rho_g = b_g * (bar[rho_g] / bar[rho_m]) * (rho_m - bar[rho_m]) + bar[rho_g]\n",
    "# rho_m = KDE\n",
    "# bar[rho_m] = 1 / atot (assuming bubble has average density overall)\n",
    "# bar[rho_g] = N_GAMA / A_GAMA\n",
    "\n",
    "# # constant linear bias factor\n",
    "b_g = 1.5\n",
    "\n",
    "# bias_thresh_rel = b_g * (dens_rel * (A_SLICS / 256**2))\n",
    "# print(bias_thresh_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_kde(key):\n",
    "    basefn = key\n",
    "    (i, j) = bubble_combos[key]\n",
    "\n",
    "    xran = pd.read_csv(os.path.join(basefn, 'ygrid.csv')).to_numpy()[:, 1]\n",
    "    yran = pd.read_csv(os.path.join(basefn, 'xgrid.csv')).to_numpy()[:, 1]\n",
    "    allpos = np.meshgrid(xran, yran)\n",
    "    xmin, xmax = xran[0], xran[-1]\n",
    "    ymin, ymax = yran[0], yran[-1]\n",
    "    atot = (ymax - ymin) * (xmax - xmin)\n",
    "    da = atot / ((res - 1) * (res - 1))\n",
    "\n",
    "    kde = pd.read_csv(os.path.join(basefn, 'kdegrid.csv')).to_numpy().T[1:].T\n",
    "    \n",
    "#     plt.imshow(kde, extent=[xmin, xmax, ymin, ymax])\n",
    "#     plt.title(key[20:])\n",
    "#     plt.savefig(os.path.join(basefn, 'kdegrid.png'))\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "    \n",
    "#     npart = len(pd.read_csv(os.path.join(basefn, 'projection.csv')))\n",
    "#     dens_mock = npart / (angbox ** 2)\n",
    "#     footprint_contrast = (dens_mock - avg_dens_SLICS) / avg_dens_SLICS\n",
    "    \n",
    "    kdenorm = kde * da\n",
    "    kdetot = np.sum(kdenorm)\n",
    "    assert(np.isclose(kdetot, 1.))\n",
    "    kdeavg = np.mean(kdenorm)\n",
    "    linearized = np.cumsum(kdenorm.reshape((1, res * res)))\n",
    "    kde_contrast = (kdenorm - kdeavg) / kdeavg\n",
    "    # TODO: maybe write a function to calculate density contrasts?\n",
    "    \n",
    "#     print((key, footprint_contrast, kde_contrast))\n",
    "    \n",
    "    \n",
    "    plt.hist(kde_contrast.flatten(), density=True)\n",
    "    plt.vlines(b_g - 1., 0., 50.)\n",
    "    plt.title(str(key))\n",
    "    plt.savefig(os.path.join(basefn, 'kde_dens_contrast.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    allpatch = []\n",
    "    for l in range(npatch):\n",
    "        ngal = int(atot * allz_gama_dens[i][l])\n",
    "#         print(ngal)\n",
    "        galpos = []\n",
    "        while len(galpos) < ngal:\n",
    "            rando = random.random()\n",
    "            loc = bisect.bisect(linearized, rando)\n",
    "            galind = np.unravel_index(loc, (res, res))\n",
    "        # this should be (kdenorm[galind] - slicsavgdens) / slicsavgdens * gamapatchavgdens >= b_g\n",
    "        # will have to account for mass of particles vs galaxies!\n",
    "            if kde_contrast[galind] >= b_g - 1.:\n",
    "#             if kdenorm[galind] >= b_g * kde_contrast * \n",
    "            # NOTE: instead, could maybe use position of a particle in the drawn cell\n",
    "                galpos.append([allpos[0][galind], allpos[1][galind]])#[allpos[0][np.int(loc / res)][np.mod(loc, res)], allpos[1][np.int(loc / res)][np.mod(loc, res)]])\n",
    "        \n",
    "        galpos = np.array(galpos)\n",
    "#         galposinds = np.unravel_index(galinds, (res, res))\n",
    "#         galpos = allpos[galinds]\n",
    "        np.savetxt(os.path.join(basefn, 'patch'+str(l)+'mockpos'+str(b_g)+'.csv'), galpos)\n",
    "        allpatch.append(galpos)\n",
    "    return(allpatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(nps)\n",
    "allpatches = pool.map(sample_kde, bubble_combos.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: plot the sampled mock galaxy positions on top of KDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "benchmarking test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmin = 5\n",
    "benchmark_combos = {}\n",
    "for i in range(len(z_SLICS)):\n",
    "    for j in [38]:#21, 22, 25, 26, 37, 38, 41, 42]:\n",
    "        boxpath = pathname+'/z'+str(i)+'/box'+str(j)\n",
    "        one_ang = pd.read_csv(boxpath+'/projection.csv')\n",
    "        npart_oom = int(np.floor(np.log10(len(one_ang))))\n",
    "        k = kmin\n",
    "        while k <= npart_oom:\n",
    "            samppath = boxpath+'/samp'+str(k)\n",
    "            if os.path.isdir(samppath) == False:\n",
    "                os.system('mkdir '+ samppath)\n",
    "            benchmark_combos[samppath] = (i, j, k)#+'/patch'+str(k)] = (i, j, l, k)\n",
    "            subsamp = one_ang.sample(10**k)\n",
    "            subsamp.to_csv(os.path.join(samppath, 'projection.csv'), index=False)\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_kde(one_key):\n",
    "    (i, j, k) = benchmark_combos[one_key]\n",
    "    ro.r('dataRAW <- read.csv(\"'+one_key+'/projection.csv'+'\")')\n",
    "    \n",
    "# try data.table's fread\n",
    "\n",
    "# get single columns from data frame\n",
    "    ro.r('x <- dataRAW$RA')\n",
    "    ro.r('y <- dataRAW$DEC')\n",
    "\n",
    "# put in the correct format\n",
    "    ro.r('myPointData <- ppp(x, y, xrange=range(x), yrange=range(y))')\n",
    "    \n",
    "# read variables into R section\n",
    "    ro.r('h0 = '+str(bw_ival[i]))\n",
    "    ro.r('resolution = '+str(res))#4096\n",
    "    ro.r('use.ppp.methods = TRUE')\n",
    "    \n",
    "    ti = time.time()\n",
    "    ro.r('ddest <- bivariate.density(myPointData, h0=h0, adapt=TRUE, resolution=resolution)')\n",
    "# # consider use.ppp.methods = TRUE\n",
    "    # ro.r('ddest <- multiscale.density(myPointData, h0=h0)')\n",
    "    tf = time.time()\n",
    "    dt = tf - ti\n",
    "    message = one_key+' kde at '+str(res)+'x'+str(res)+' completed in '+str(dt)+' seconds\\n'\n",
    "    print(message)\n",
    "\n",
    "    # ro.r('write.csv(ddest$z$v, \"'+kdefn+'\")')\n",
    "    for key in to_save.keys():\n",
    "        ro.r('write.csv(ddest'+key+', \"'+one_key+'/'+to_save[key]+'.csv\")')\n",
    "    \n",
    "    with open(os.path.join(pathname, 'benchmark.txt'), 'a') as stdout:\n",
    "        stdout.write(message)\n",
    "        # write essential output to files\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = mp.Pool(nps)\n",
    "# pool.map(benchmark_kde, benchmark_combos.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note resolution needs to be scaled for angular area!\n",
    "def what_is_bubble(loc_data, xname, yname, zbin, bins=int(resolution)):\n",
    "#     plt.hist2d(loc_data[xname], loc_data[yname], bins=(bins, bins), norm=mpl.colors.LogNorm(), cmap='Spectral_r')\n",
    "#     plt.show()\n",
    "    (coarsedensity, xedges, yedges) = np.histogram2d(loc_data[xname], loc_data[yname], bins=bins)\n",
    "    extreme = np.quantile(coarsedensity.flatten(), 0.99)\n",
    "    indcenters = np.argwhere(coarsedensity > extreme)\n",
    "#     print(indcenters)\n",
    "    #NOTE: finedensity step would go here\n",
    "    xcenters = (xedges[indcenters.T[0]] + xedges[indcenters.T[0]+1]) / 2\n",
    "    ycenters = (yedges[indcenters.T[1]] + yedges[indcenters.T[1]+1]) / 2\n",
    "#     print('identified bubble centers, going through each bubble')\n",
    "    minscale = 10.e6 * ap.units.pc / d_comov[zbin] * 180 / np.pi\n",
    "    bubbles = []\n",
    "    for i, center in enumerate(indcenters):\n",
    "        bubble = loc_data.loc[lambda df: (df[xname] > xcenters[i] - minscale) & (df[xname] < xcenters[i] + minscale)\n",
    "                          & (df[yname] > ycenters[i] - minscale) & (df[yname] < ycenters[i] + minscale), :]\n",
    "#         if ~bubble.duplicated().any():\n",
    "#             print('no duplicate coordinates in x, y, z, vx, vy, vz')\n",
    "#         print((len(loc_data), len(bubble)))\n",
    "#         plt.hist2d(bubble[xname], bubble[yname], bins=(bins, bins), norm=mpl.colors.LogNorm(), cmap='Spectral_r')\n",
    "#         plt.show()\n",
    "        bubbles.append(bubble)\n",
    "    return(bubbles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allz_gama_dens[1][2] / dens_SLICS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of mock galaxy samples derived empirically based on maximizing coverage with GAMA environment curves: take a lot of sample positions and recalculate environment curves with subsets until they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basefn = pathname+'/z'+str(0)+'/box'+str(38)+'/'\n",
    "rel_part = len(pd.read_csv(basefn+'particles.csv')) / N_slice\n",
    "xran = pd.read_csv(basefn+'ygrid.csv').to_numpy()[:, 1]\n",
    "yran = pd.read_csv(basefn+'xgrid.csv').to_numpy()[:, 1]\n",
    "xmin, xmax = xran[0], xran[-1]\n",
    "ymin, ymax = yran[0], yran[-1]\n",
    "atot = (ymax - ymin) * (xmax - xmin) * (ap.units.degree)**2\n",
    "rel_area = atot / ang_area_SLICS[0].value\n",
    "rel_dens = rel_part / rel_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rel_part)\n",
    "print(rel_area)\n",
    "print(rel_dens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_SLICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_contra_slics = (len(pd.read_csv(basefn+'particles.csv')) * M_part / atot - dens_SLICS[0]) / dens_SLICS[0]\n",
    "print(dens_contra_slics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_patch(one_key):\n",
    "#     (chosenz, chosenbox, chosenbubble, chosenpatch) = bubble_combos[one_key]\n",
    "    (i, j, k) = bubble_combos[one_key]\n",
    "    basefn = pathname+'/z'+str(i)+'/box'+str(j)+'/bub'+str(k)+'/'\n",
    "    \n",
    "    normfactor = len(np.loadtxt(basefn+'particles.csv'))\n",
    "    xran = pd.read_csv(basefn+'ygrid.csv').to_numpy()[:, 1]\n",
    "    yran = pd.read_csv(basefn+'xgrid.csv').to_numpy()[:, 1]\n",
    "    allpos = np.meshgrid(xran, yran)\n",
    "    \n",
    "    xmin, xmax = xran[0], xran[-1]\n",
    "    ymin, ymax = yran[0], yran[-1]\n",
    "    atot = (ymax - ymin) * (xmax - xmin)\n",
    "    da = atot / ((res - 1) * (res - 1))\n",
    "    \n",
    "    kde = pd.read_csv(basefn+'kdegrid.csv').to_numpy().T[1:].T\n",
    "    kdenorm = kde * da\n",
    "    kdetot = np.sum(kdenorm)\n",
    "    assert(np.isclose(kdetot, 1.))\n",
    "    kdeavg = np.mean(kdenorm)\n",
    "    \n",
    "    linearized = np.cumsum(kdenorm.reshape((1, res * res)))\n",
    "    # attempting to implement galaxy bias here\n",
    "#     biased_linearized = linearized * b_g\n",
    "    \n",
    "    for l in range(npatch):\n",
    "        ngal = 1e5#int(atot * allz_gama_dens[i][l])\n",
    "        galpos = []\n",
    "        while len(galpos) < ngal:\n",
    "            rando = random.random()\n",
    "            loc = bisect.bisect(linearized, rando)\n",
    "            galind = np.unravel_index(loc, (res, res))\n",
    "            # this should be (kdenorm[galind] - slicsavgdens) / slicsavgdens * gamapatchavgdens >= b_g\n",
    "            # will have to account for mass of particles vs galaxies!\n",
    "            if (kdenorm[galind] - kdeavg) / kdeavg >= b_g:\n",
    "                # could maybe use position of a particle in the drawn cell\n",
    "                galpos.append([allpos[0][galind], allpos[1][galind]])#[allpos[0][np.int(loc / res)][np.mod(loc, res)], allpos[1][np.int(loc / res)][np.mod(loc, res)]])\n",
    "        galpos = np.array(galpos).T\n",
    "#         galposinds = np.unravel_index(galinds, (res, res))\n",
    "#         galpos = allpos[galinds]\n",
    "#         print(np.shape(galpos))\n",
    "        if os.path.isdir(basefn+'patch'+str(l)) == False:\n",
    "            os.system('mkdir '+ basefn+'patch'+str(l))\n",
    "        np.savetxt(basefn+'patch'+str(l)+'/mockpos'+str(b_g)+'.csv', galpos)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(nps)\n",
    "pool.map(one_patch, bubble_combos.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobias = np.loadtxt('DEAR/Data/bubbles/z4/box37/bub2/patch2/mockpos1.0.csv')\n",
    "bias = np.loadtxt('DEAR/Data/bubbles/z4/box37/bub2/patch2/mockpos1.5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(nobias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(nobias[0], nobias[1], c='b', alpha=0.005, s=1)\n",
    "plt.scatter(bias[0], bias[1], c='r', alpha=0.005, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biashist = np.histogram2d(bias[0], bias[1], bins=res)\n",
    "nobiashist = np.histogram2d(nobias[0], nobias[1], bins=res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = range(50)\n",
    "plt.hist(biashist[0].flatten(), color='r', alpha=0.5, bins=bins)\n",
    "plt.hist(nobiashist[0].flatten(), color='k', alpha=0.5, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gama = pd.read_csv('../environmet_clustering/classes/z_'+'{:<05}'.format(str(z_SLICS[1]))+'_manygroups_oneslice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gama.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allz_gama_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(allz_gama_coords[0][1]['RA_x'], allz_gama_coords[0][1]['DEC_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt('DEAR/Data/bubbles/z3/box37/bub0/patch1/mockpos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = pd.read_csv('DEAR/Data/bubbles/z3/box37/bub0/kdegrid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attic: scratch after here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: currently using bubble size based on physical scale of galaxy clusters, \n",
    "#but does need to be adjusted for angular size so it makes sense for making mock data\n",
    "dmax = 10.e6 * ap.units.pc\n",
    "rmax = dmax / h / 2.\n",
    "\n",
    "area_SLICS = (phys_scale * h * 1.e6 * ap.units.pc)**2\n",
    "ang_area_SLICS = area_SLICS / (d_ang / 180. / ap.units.degree * np.pi)**2\n",
    "dens_SLICS =  N_slice * M_part / ang_area_SLICS\n",
    "#NOTE: can use this angular area to define appropriate bubble radius/kde resolution relationship\n",
    "\n",
    "cubeside = rnc / nodes_dim\n",
    "dmin = cubeside * rmax / (phys_scale * 1.e6 * ap.units.pc / nodes_dim)\n",
    "rmin = dmin / 2.\n",
    "\n",
    "#NOTE: this physical resolution should be replaced with an angular-motivated bubble size resolution (in physical units)\n",
    "resolution = (phys_scale * 1.e6 * ap.units.pc / nodes_dim) / rmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #make bubbles of constant 2.5 degrees size\n",
    "# bub_ang = 2.5 * np.pi / 180.\n",
    "# def slics_to_ang(dphys, z): \n",
    "#     phys = dphys * ap.units.pc\n",
    "#     dc = cosmo.comoving_distance(float(z))\n",
    "#     da = dc / (1 + z)\n",
    "#     ang = (phys / da) * (180. / np.pi)\n",
    "#     return ang\n",
    "    \n",
    "# def ang_to_phys(ang, z):\n",
    "#     def helper(phys):\n",
    "#         angres = slics_to_ang(phys, z)\n",
    "#         dif = np.abs(ang - angres)\n",
    "#         return(dif)\n",
    "#     res = spo.minimize(helper, boxside/(1+z))\n",
    "#     frac_box = res.x * ap.units.pc / boxside\n",
    "#     return frac_box\n",
    "\n",
    "# ang_to_phys(2.5, z_SLICS[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recidivator (Python 3)",
   "language": "python",
   "name": "recidivator_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
