{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flexible environment curve generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy as ap\n",
    "from astropy import coordinates\n",
    "import bisect\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many cores I'm allowed to use q-:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nps = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isolate usable sample from GAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toppath = '/media/CRP6/Cosmology/recidivator/GAMA/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split by field for speed, omit field with nonuniform coverage\n",
    "\n",
    "sky coordinate limits came from [GAMA website](https://www.astro.ljmu.ac.uk/~ikb/research/gama_fields/) -- must include DEC for this to work!!!\n",
    "\n",
    "NOTE: this fails on G15 field because it's got too many galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = {}\n",
    "xlim['G02'] = (30.2, 38.8)\n",
    "xlim['G09'] = (129.0, 141.0)\n",
    "xlim['G12'] = (174.0, 186.0)\n",
    "xlim['G15'] = (211.5, 223.5)\n",
    "\n",
    "ylim = {}\n",
    "ylim['G02'] = (-10.5, -3.72)\n",
    "ylim['G09'] = (-2., 3.)\n",
    "ylim['G12'] = (-3., 2.)\n",
    "ylim['G15'] = (-2., 3.)\n",
    "\n",
    "maxang = ap.units.Quantity(2.5, 'deg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter for data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zlim = (0., 0.5)\n",
    "newpath = '/media2/CRP6/Cosmology/envirocurves/GAMA/'\n",
    "\n",
    "all_q = pd.read_csv(toppath+'test/one_slice/SpecObjPhot.csv', index_col=['CATAID'])\n",
    "high_q = all_q[all_q['NQ'] > 2]\n",
    "high_q = high_q.rename(columns={\"Unnamed: 0\": \"SpecObjPhot_index\"})\n",
    "\n",
    "gf = {}\n",
    "for key in xlim.keys():\n",
    "    gf[key] = high_q.loc[(high_q['RA'] > xlim[key][0]) & (high_q['RA'] < xlim[key][1]) & \n",
    "                         (high_q['DEC'] > ylim[key][0]) & (high_q['DEC'] < ylim[key][1]) & \n",
    "                         (high_q['Z'] > zlim[0]) & (high_q['Z'] < zlim[1])]\n",
    "    \n",
    "n_gal_kept = {}\n",
    "for key in gf.keys():\n",
    "    n_gal_kept[key] = len(gf[key])\n",
    "    gf[key].to_csv(newpath+'field'+key+'high_q.csv')\n",
    "# print(n_gal_kept)\n",
    "\n",
    "# easy_inds = {}\n",
    "# for key in gf.keys():\n",
    "#     easy_inds[key] = pd.read_csv(toppath+'flexible_envirocurves/field'+key+'high_q.csv')\n",
    "#     print((key, len(easy_inds[key])))\n",
    "#     easy_inds[key].index.rename('field_index', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "break down files as needed (fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max number of galaxies per chunked file\n",
    "maxfile = 2**14\n",
    "share_index = range(maxfile)\n",
    "print(maxfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_down_neighbor_input(keyno):\n",
    "    big_in = pd.read_csv(newpath+'field'+field+'high_q.csv', index_col=['CATAID'])\n",
    "    big_out = big_in.copy()\n",
    "    nbig = len(big_in)# total number of galaxies in big file\n",
    "    nfile = 0\n",
    "    paircount = []\n",
    "    while nfile * maxfile <= nbig:\n",
    "        glob_ind_min = nfile * maxfile# min real index for this chunk\n",
    "        glob_ind_max = (nfile + 1) * maxfile\n",
    "        if nbig < glob_ind_max:\n",
    "            chunk_ind_max = nbig - glob_ind_min\n",
    "            glob_ind_max = nbig\n",
    "        else:\n",
    "            chunk_ind_max = maxfile\n",
    "        paircount.append((glob_ind_min, glob_ind_max))\n",
    "        small_piece = big_in[glob_ind_min : glob_ind_max]\n",
    "        big_out['chunk_'+str(nfile)] = np.nan\n",
    "        big_out['chunk_'+str(nfile)][glob_ind_min : glob_ind_max] = share_index[:chunk_ind_max]\n",
    "        small_piece.to_csv(newpath+field+'chunk'+str(nfile)+'.csv')\n",
    "        print('separated out chunk '+str(nfile)+' of '+field+': '+str(paircount[-1]))\n",
    "        nfile += 1\n",
    "#     paircount = nfile\n",
    "    big_out.to_csv(newpath+field+'allchunks.csv')\n",
    "    minx, maxx = min(big_in['RA']), max(big_in['RA'])\n",
    "    miny, maxy = min(big_in['DEC']), max(big_in['DEC'])\n",
    "    extrema = (minx, maxx, miny, maxy)\n",
    "    return nfile, paircount, extrema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in xlim.keys():\n",
    "#     nps = 1#len(gf.keys()) #mp.cpu_count()\n",
    "    pool = mp.Pool(nps)\n",
    "    paircounts = pool.map(break_down_neighbor_input, range(1))#range(len(gf.keys())))\n",
    "    # print(paircounts)\n",
    "    with open(newpath+field+'chunkinfo.csv', 'wb') as chunkinfo:\n",
    "        pkl.dump(paircounts, chunkinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testfile = pd.read_csv(toppath+'flexible_envirocurves/'+'G15'+'allchunks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testfile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testfile[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the neighbors within max angular distance set by GAMA footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_neighbors(keyno):\n",
    "#     field = list(gf.keys())[keyno]\n",
    "#     small_piece = pd.read_csv(toppath+'flexible_envirocurves/field'+field+'high_q.csv')\n",
    "# #     print(small_piece.iloc[99])\n",
    "#     coords = ap.coordinates.SkyCoord(small_piece['RA'], small_piece['DEC'], unit='deg')\n",
    "#     #warning: slow!!! (not actually that slow)\n",
    "#     output = ap.coordinates.search_around_sky(coords, coords, maxang)\n",
    "#     with open(toppath+'flexible_envirocurves/neighbors_'+field+'_allz.pkl', 'wb') as outfile:\n",
    "#         pkl.dump(output, outfile)\n",
    "#     return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #still kinda slow\n",
    "# nps = len(gf.keys()) #mp.cpu_count()\n",
    "# pool = mp.Pool(nps)\n",
    "# pool.map(get_neighbors, range(len(gf.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is prep for the slow step. Have to do separately (not in a loop) for each field due to memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allfields = xlim.keys()\n",
    "field = 'G15'#change me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(newpath+field+'chunkinfo.csv', 'rb') as chunkinfo:\n",
    "    paircounts = pkl.load(chunkinfo)\n",
    "\n",
    "nfile = paircounts[0][0]\n",
    "indends = paircounts[0][1]\n",
    "(minx, maxx, miny, maxy) = paircounts[0][2]\n",
    "\n",
    "chunk_pairs = {}\n",
    "pairkey = 0\n",
    "for i in range(nfile):\n",
    "    for j in range(nfile):\n",
    "        chunk_pairs[pairkey] = (i, j)\n",
    "        pairkey += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors_breakup(keyno):\n",
    "    (a, b) = chunk_pairs[keyno]\n",
    "    small_a = pd.read_csv(newpath+field+'chunk'+str(a)+'.csv', index_col=['CATAID'])\n",
    "    small_b = pd.read_csv(newpath+field+'chunk'+str(b)+'.csv', index_col=['CATAID'])\n",
    "#     big_piece = pd.read_csv(toppath+'flexible_envirocurves/LOS'+field+'inputs.csv', index_col=['CATAID'])\n",
    "#     small_piece = break_down_neighbor_input(big_piece)\n",
    "#     print(small_piece.iloc[99])\n",
    "    coords_a = ap.coordinates.SkyCoord(small_a['RA'], small_a['DEC'], unit='deg')\n",
    "    coords_b = ap.coordinates.SkyCoord(small_b['RA'], small_b['DEC'], unit='deg')\n",
    "    #warning: slow!!! (not actually that slow)\n",
    "#     print('started finding neighbors')\n",
    "    output = ap.coordinates.search_around_sky(coords_a, coords_b, maxang)\n",
    "#     print('finished finding neighbors')\n",
    "    filepath = newpath+'neighbors_'+field+'chunk'+str(a)+'Xchunk'+str(b)+'allz.pkl'\n",
    "    pkl.dump(output, open(filepath, 'wb'))\n",
    "    print(filepath)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the slow step!!! Repeat for each field to avoid memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for field in xlim.keys():\n",
    "#     get_neighbors_one_field(field)\n",
    "# nps = 25#pairkey #mp.cpu_count()\n",
    "pool = mp.Pool(nps)\n",
    "pool.map(get_neighbors_breakup, range(pairkey))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse neighbor info and evaluate at distances\n",
    "\n",
    "memory problems if doing these steps separately, sadly slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with many more choices for this!\n",
    "ndist = 100# 128\n",
    "# distance_evaluation_points = np.exp(np.linspace(np.log(2.5 / 60. / 60.), np.log(2.5), ndist+2)[1:-1])\n",
    "distance_evaluation_points = np.linspace(0., maxang.value, ndist+2)[1:-1]\n",
    "# pos = bisect.bisect(distance_evaluation_points, maxang.value)\n",
    "# distance_evaluation_points = distance_evaluation_points[:pos]\n",
    "# n_dists = len(distance_evaluation_points)\n",
    "# iter_over_dists = enumerate(distance_evaluation_points)\n",
    "# ndist = range(len(distance_evaluation_points))\n",
    "# with open(toppath+'flexible_envirocurves/eval'+str(ndist)+'dists_LOS'+field+'.pkl', 'wb') as savedists:\n",
    "#     pkl.dump(distance_evaluation_points, savedists)\n",
    "# print(distance_evaluation_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(r, d, theta=None):\n",
    "    if theta == None:\n",
    "        theta = 2. * np.arccos(d / r)\n",
    "    return r**2 * (theta - np.sin(theta)) / 2.\n",
    "\n",
    "def sector(r, d, theta=None):\n",
    "    if theta == None:\n",
    "        theta = np.arcsin(d / r)\n",
    "    return r**2 * theta / 2.\n",
    "\n",
    "# this throws an error at the points used to define minx, maxx, miny, maxy\n",
    "def area(r, x, y, minx, maxx, miny, maxy, vb=True):\n",
    "    lx = x - minx\n",
    "    ux = maxx - x\n",
    "    ly = y - miny\n",
    "    uy = maxy - y\n",
    "    distances = np.array([lx, ux, ly, uy])\n",
    "    #print(distances)\n",
    "    condition = (distances >= r)\n",
    "    ntrue = sum(condition)\n",
    "    if ntrue == 4:\n",
    "        return np.pi * r**2\n",
    "    elif ntrue == 3:\n",
    "        return np.pi * r**2 - segment(r, min(distances))\n",
    "    elif ntrue == 2:\n",
    "        if vb: print('radii should be chosen so that these cannot be parallel, \\\n",
    "                but will at some point add in a check for this')\n",
    "        distx = min(distances[:2])\n",
    "        disty = min(distances[-2:])\n",
    "        if np.sqrt(distx**2 + disty**2) < r:\n",
    "            thetax = np.arcsin(distx / r)\n",
    "            thetay = np.arcsin(disty / r)\n",
    "            areax = distx * r * np.cos(thetax) / 2.\n",
    "            areay = disty * r * np.cos(thetay) / 2.\n",
    "            return sector(r, distx, theta=thetax) + sector(r, disty, theta=thetay) + \\\n",
    "                            sector(r, r, theta=np.pi / 2.) + distx * disty + areax + areay\n",
    "        else:\n",
    "            return np.pi * r**2 - segment(r, distx) - segment(r, disty)\n",
    "    else:\n",
    "        if vb: print('this case should not happen because we did not consider radii \\\n",
    "                beyond half the shortest side of the footprint,\\\n",
    "                but will at some point deal with this case')\n",
    "        return None\n",
    "\n",
    "\n",
    "# ## Calculates volume normalized environment\n",
    "# def calc_env(ind):\n",
    "#     \"\"\"\n",
    "#         Runs galenv to calculate galaxy environment.\n",
    "#         This is set up to run in the multiprocessing so a lot of inputs are\n",
    "#         not set when you call the function, but are supposed to be defined\n",
    "#         when running this code.\n",
    "\n",
    "#         Output: nearest neighbors at a given angular separation.\n",
    "#     \"\"\"\n",
    "#     if opts.run_environment:\n",
    "#         # Generates environments for GAMA RA/Dec data\n",
    "#         res = [subsamples[f][s]['CATAID'].values[ind]]\n",
    "#     if opts.run_particle_environment:\n",
    "#         # Generates environments for particle RA/Dec data\n",
    "#         res = [ind]\n",
    "\n",
    "#     friends = data\n",
    "#     for dist in try_distances:\n",
    "#         friends = galenv.nn_finder(friends, data[ind], dist)\n",
    "#         #print('r/dist', dist, 'x ', data[ind][0], 'y ', data[ind][1], 'minx', minx, 'maxx', maxx, 'miny', miny, 'maxy', maxy)\n",
    "#         vol = area(dist, data[ind][0], data[ind][1], minx, maxx, miny, maxy, vb=False)\n",
    "#         #print('vol', vol)\n",
    "#         res.append(float(len(friends)) / vol)\n",
    "#     return res\n",
    "# ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again, have to run separately per field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allfields = xlim.keys()\n",
    "# field = allfields[0]#change me!\n",
    "loskey = field\n",
    "\n",
    "with open(newpath+field+'chunkinfo.csv', 'rb') as chunkinfo:\n",
    "    paircounts = pkl.load(chunkinfo)\n",
    "\n",
    "nfile = paircounts[0][0]\n",
    "indends = paircounts[0][1]\n",
    "(minx, maxx, miny, maxy) = paircounts[0][2]\n",
    "\n",
    "chunk_pairs = {}\n",
    "pairkey = 0\n",
    "for i in range(nfile):\n",
    "    for j in range(nfile):\n",
    "        chunk_pairs[pairkey] = (i, j)\n",
    "        pairkey += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(nfile):\n",
    "    small_piece = pd.read_csv(newpath+loskey+'chunk'+str(i)+'.csv', index_col='CATAID')\n",
    "    which_dists = distance_evaluation_points\n",
    "    for k, dist in enumerate(which_dists):\n",
    "        small_piece[str(dist)] = np.zeros_like(len(small_piece))\n",
    "    for j in range(nfile):\n",
    "        with open(newpath+'neighbors_'+loskey+'chunk'+str(i)+'Xchunk'+str(j)+'allz.pkl', 'rb') as tomerge:\n",
    "            all_pairs = pkl.load(tomerge)\n",
    "        inds_with_neighbors, where_my_neighbors_at = np.unique(all_pairs[0], return_inverse=True)\n",
    "        def help_gather_neighbors(ind):\n",
    "            newrow = np.zeros_like(which_dists)\n",
    "            if ind in inds_with_neighbors:\n",
    "                my_neighbor_dists = all_pairs[2][where_my_neighbors_at == ind].value\n",
    "                sort_dists = sorted(list(my_neighbor_dists))\n",
    "                for k, dist in enumerate(which_dists):\n",
    "                    newrow[k] = bisect.bisect(sort_dists, dist)\n",
    "            return(newrow)\n",
    "#         nps = 25#mp.cpu_count()\n",
    "        pool = mp.Pool(nps)\n",
    "        distcounts = pool.map(help_gather_neighbors, range(len(small_piece.index)))\n",
    "        distcounts = np.array(distcounts).T\n",
    "        print(str((i, j, np.shape(distcounts))))\n",
    "        for k, dist in enumerate(which_dists):\n",
    "            small_piece[str(dist)] += distcounts[k]\n",
    "    for k, dist in enumerate(which_dists):\n",
    "        small_piece[str(dist)] = small_piece.apply(lambda row: float(row[str(dist)]) / \n",
    "                                                        area(dist, row['RA'], row['DEC'], \n",
    "                                                             minx, maxx, miny, maxy, vb=False), axis=1)\n",
    "    print(small_piece)\n",
    "    small_piece.to_csv(newpath+loskey+'chunk'+str(i)+'dists'+str(ndist)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no longer used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD: parse the neighbor info to flexibly make curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add in redshift filtering here or earlier?\n",
    "\n",
    "# def help_find_neighbors(ind):\n",
    "#     where_my_neighbors_at = np.where(all_pairs[0] == ind)\n",
    "#     my_neighbor_inds = all_pairs[1][where_my_neighbors_at]\n",
    "#     save_df = df[['CATAID', 'RA', 'DEC', 'Z']].iloc[my_neighbor_inds]\n",
    "# #     my_neighbor_zs = df.iloc[my_neighbor_inds]['Z']\n",
    "#     neighbor_dists = all_pairs[2][where_my_neighbors_at]\n",
    "#     save_df['dist'+str(ind)] = neighbor_dists\n",
    "# #     nn = len(neighbor_distances)\n",
    "# #     res[ind] = sorted(neighbor_distances)\n",
    "#     output = (ind, df.iloc[ind]['CATAID'], save_df)\n",
    "# #     res[ind] = output\n",
    "#     return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_one_field(test_key):\n",
    "#     df = pd.read_csv(toppath+'flexible_envirocurves/field'+test_key+'high_q.csv')\n",
    "\n",
    "#     with open(toppath+'flexible_envirocurves/neighbors_'+test_key+'_allz.pkl', 'rb') as pairfile:\n",
    "#         all_pairs = pkl.load(pairfile)\n",
    "\n",
    "#         # res = [[]] * len(df.index)\n",
    "#     nps = 35#mp.cpu_count() - 1\n",
    "#     pool = mp.Pool(nps)\n",
    "#     try_neighbor_dists = pool.map(help_find_neighbors, df.index)\n",
    "#     pkl.dump(try_neighbor_dists, open(toppath+'flexible_envirocurves/parsed_neighbor_dists_'+test_key+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for test_key in gf.keys():\n",
    "#     parse_one_field(test_key)\n",
    "#     print(test_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD: now evaluate at distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try with many more choices for this!\n",
    "# ndist = 100# 128\n",
    "# # distance_evaluation_points = np.exp(np.linspace(np.log(2.5 / 60. / 60.), np.log(2.5), ndist+2)[1:-1])\n",
    "# distance_evaluation_points = np.linspace(0., maxang.value, ndist+2)[1:-1]\n",
    "# # n_dists = len(distance_evaluation_points)\n",
    "# # iter_over_dists = enumerate(distance_evaluation_points)\n",
    "# # ndist = range(len(distance_evaluation_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndist = 10#100\n",
    "# # distance_evaluation_points = np.exp(np.linspace(np.log(2.5 / 60. / 60.), np.log(2.5), ndist+2)[1:-1])\n",
    "# distance_evaluation_points = np.linspace(0., 1., ndist+2)[1:-1]\n",
    "# pos = bisect.bisect(distance_evaluation_points, maxang.value)\n",
    "# distance_evaluation_points = distance_evaluation_points[:pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize the curves by neighbors-per-area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def segment(r, d, theta=None):\n",
    "#     if theta == None:\n",
    "#         theta = 2. * np.arccos(d / r)\n",
    "#     return r**2 * (theta - np.sin(theta)) / 2.\n",
    "\n",
    "# def sector(r, d, theta=None):\n",
    "#     if theta == None:\n",
    "#         theta = np.arcsin(d / r)\n",
    "#     return r**2 * theta / 2.\n",
    "\n",
    "# # this throws an error at the points used to define minx, maxx, miny, maxy\n",
    "# def area(r, x, y, minx, maxx, miny, maxy, vb=True):\n",
    "#     lx = x - minx\n",
    "#     ux = maxx - x\n",
    "#     ly = y - miny\n",
    "#     uy = maxy - y\n",
    "#     distances = np.array([lx, ux, ly, uy])\n",
    "#     #print(distances)\n",
    "#     condition = (distances >= r)\n",
    "#     ntrue = sum(condition)\n",
    "#     if ntrue == 4:\n",
    "#         return np.pi * r**2\n",
    "#     elif ntrue == 3:\n",
    "#         return np.pi * r**2 - segment(r, min(distances))\n",
    "#     elif ntrue == 2:\n",
    "#         if vb: print('radii should be chosen so that these cannot be parallel, \\\n",
    "#                 but will at some point add in a check for this')\n",
    "#         distx = min(distances[:2])\n",
    "#         disty = min(distances[-2:])\n",
    "#         if np.sqrt(distx**2 + disty**2) < r:\n",
    "#             thetax = np.arcsin(distx / r)\n",
    "#             thetay = np.arcsin(disty / r)\n",
    "#             areax = distx * r * np.cos(thetax) / 2.\n",
    "#             areay = disty * r * np.cos(thetay) / 2.\n",
    "#             return sector(r, distx, theta=thetax) + sector(r, disty, theta=thetay) + \\\n",
    "#                             sector(r, r, theta=np.pi / 2.) + distx * disty + areax + areay\n",
    "#         else:\n",
    "#             return np.pi * r**2 - segment(r, distx) - segment(r, disty)\n",
    "#     else:\n",
    "#         if vb: print('this case should not happen because we did not consider radii \\\n",
    "#                 beyond half the shortest side of the footprint,\\\n",
    "#                 but will at some point deal with this case')\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # ## Calculates volume normalized environment\n",
    "# # def calc_env(ind):\n",
    "# #     \"\"\"\n",
    "# #         Runs galenv to calculate galaxy environment.\n",
    "# #         This is set up to run in the multiprocessing so a lot of inputs are\n",
    "# #         not set when you call the function, but are supposed to be defined\n",
    "# #         when running this code.\n",
    "\n",
    "# #         Output: nearest neighbors at a given angular separation.\n",
    "# #     \"\"\"\n",
    "# #     if opts.run_environment:\n",
    "# #         # Generates environments for GAMA RA/Dec data\n",
    "# #         res = [subsamples[f][s]['CATAID'].values[ind]]\n",
    "# #     if opts.run_particle_environment:\n",
    "# #         # Generates environments for particle RA/Dec data\n",
    "# #         res = [ind]\n",
    "\n",
    "# #     friends = data\n",
    "# #     for dist in try_distances:\n",
    "# #         friends = galenv.nn_finder(friends, data[ind], dist)\n",
    "# #         #print('r/dist', dist, 'x ', data[ind][0], 'y ', data[ind][1], 'minx', minx, 'maxx', maxx, 'miny', miny, 'maxy', maxy)\n",
    "# #         vol = area(dist, data[ind][0], data[ind][1], minx, maxx, miny, maxy, vb=False)\n",
    "# #         #print('vol', vol)\n",
    "# #         res.append(float(len(friends)) / vol)\n",
    "# #     return res\n",
    "# # ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vols = np.empty((ndist))\n",
    "# for i, dist in enumerate(distance_evaluation_points):\n",
    "#     vols[i] = area(dist, ra, dec, minx, maxx, miny, maxy, vb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_curve_one_field(test_key):\n",
    "\n",
    "#     minx = xlim[test_key][0]\n",
    "#     maxx = xlim[test_key][1]\n",
    "#     miny = ylim[test_key][0]\n",
    "#     maxy = ylim[test_key][1]\n",
    "\n",
    "# #     df = pd.read_csv(toppath+'flexible_envirocurves/field'+test_key+'high_q.csv')\n",
    "\n",
    "# #     if find_neighbors:\n",
    "# #         all_pairs = pkl.load(open(toppath+'flexible_envirocurves/neighbors_'+test_key+'_allz.pkl', 'rb'))\n",
    "\n",
    "# #         # res = [[]] * len(df.index)\n",
    "# #         nps = 35#mp.cpu_count() - 1\n",
    "# #         pool = mp.Pool(nps)\n",
    "# #         try_neighbor_dists = pool.map(help_find_neighbors, df.index)\n",
    "# #         pkl.dump(try_neighbor_dists, open(toppath+'flexible_envirocurves/parsed_neighbor_dists_'+test_key+'.pkl', 'wb'))\n",
    "    \n",
    "#     the_neighborhood = pkl.load(open(toppath+'flexible_envirocurves/parsed_neighbor_dists_'+test_key+'.pkl', 'rb'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the_neighborhood = pkl.load(open(toppath+'flexible_envirocurves/parsed_neighbor_dists_'+test_key+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_key = 'G12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xpl = np.linspace(0., 2.5, ndist+2)[1:-1]\n",
    "# plt.scatter(xpl, np.linspace(0., 2.5, ndist+2)[1:-1])\n",
    "# plt.scatter(xpl, np.log(np.logspace(0., np.log(2.5), ndist+2)[1:-1]))\n",
    "# plt.scatter(xpl, np.exp(np.linspace(np.log(2.5 / 60. / 60.), np.log(2.5), ndist+2)[1:-1]))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also, subsample redshifts of neighbors to be near redshift of galaxy in question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next steps\n",
    "- [X] collect neighbors and distances for rudimentary curve\n",
    "- [X] variable angular distances for evaluation\n",
    "- [X] normalize for area enclosed within radius\n",
    "- [X] redshift/depth cutting\n",
    "- [ ] physical distance conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # helper to select by redshift:\n",
    "# # easiest to just go +/- 0.1\n",
    "# # then get more sophisticated\n",
    "\n",
    "# def give_redshift_neighbors(ind, delta=0.1):\n",
    "#     my_neighborhood = the_neighborhood[ind][-1]\n",
    "#     all_neighbor_zs = my_neighborhood['Z']\n",
    "#     my_z = all_neighbor_zs[ind]\n",
    "#     close_neighbor_dists = my_neighborhood['dist'+str(ind)][(all_neighbor_zs > my_z-delta) & (all_neighbor_zs < my_z+delta)]\n",
    "#     return(close_neighbor_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add in volume normalization here\n",
    "\n",
    "# def help_make_curve(ind):\n",
    "#     curve = [the_neighborhood[ind][1]]#np.empty((ndist))\n",
    "#     (ra, dec) = (df.iloc[ind]['RA'], df.iloc[ind]['DEC'])\n",
    "# #     redshifts = test_data[ind][3]\n",
    "#     distances = sorted(list(give_redshift_neighbors(ind)))\n",
    "#     for dist in distance_evaluation_points:\n",
    "# #         print(i)\n",
    "#         pos = bisect.bisect(distances, dist)\n",
    "# #         print(pos)\n",
    "# #         curve[i] = pos\n",
    "# #         all_curves.iloc[ind][str(i)] = pos\n",
    "#         vol = area(dist, ra, dec, minx, maxx, miny, maxy, vb=False)\n",
    "#         curve.append(float(pos) / vol)\n",
    "        \n",
    "#     return(curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nps = 35#mp.cpu_count() - 1\n",
    "# pool = mp.Pool(nps)\n",
    "# try_neighbor_curves = pool.map(help_make_curve, df.index)\n",
    "# newdist = np.hstack((np.ones((1)), distance_evaluation_points))\n",
    "# save_curves = np.vstack((newdist, try_neighbor_curves))\n",
    "# pkl.dump(np.array(save_curves), open(toppath+'flexible_envirocurves/zslice_normed_lin100dist_'+test_key+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thing = pkl.load(open(toppath+'flexible_envirocurves/zslice_normed_lin100dist_'+test_key+'.pkl', 'rb'))\n",
    "# thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_curves = pkl.load(open(toppath+'flexible_envirocurves/zslice_normed_curves_'+test_key+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_curves = pd.DataFrame(columns = distance_evaluation_points).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_gal = len(np.unique(test[0]))\n",
    "# sparse_arr = np.empty((n_gal, n_gal))\n",
    "# sparse_arr[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_sparse(ind):\n",
    "#     x, y = test[0][ind], test[1][ind]\n",
    "#     sparse_arr[x][y] = test[2][ind].value\n",
    "#     return sparse_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # for ind in range(len(test[0])):\n",
    "# #     fill_sparse(ind)\n",
    "# n_pair = len(test[0])\n",
    "# nps = 15\n",
    "# pool = mp.Pool(nps)\n",
    "# pool.map(fill_sparse, range(n_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_df = pd.DataFrame(sparse_arr).astype(pd.SparseDtype(\"float\", np.nan))\n",
    "# sparse_df.to_csv('sparse_neighbors_G12.csv')\n",
    "# sparse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_test = pd.DataFrame(sparse_arr).astype(pd.SparseDtype(\"float\", np.nan))\n",
    "# sparse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy_inds['G15']['neighbor_info'] = \n",
    "# easy_inds['G15']['neighbor_ids'] = []\n",
    "# def fetch_neighbors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in gf.keys():\n",
    "#     small_piece = gf[field]\n",
    "#     just_ids = small_piece['CATAID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_neighbors(field):\n",
    "#     near_dists = pkl.load(open(toppath+'flexible_envirocurves/neighbors_'+field+'_allz.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array([output[0], output[1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_q.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(all_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(high_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords1 = ap.coordinates.SkyCoord(small_piece['RA'], small_piece['DEC'], unit='deg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_piece = df.sample(200)\n",
    "# coords2 = ap.coordinates.SkyCoord(small_piece['RA'], small_piece['DEC'], unit='deg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(coords1.ra, coords1.dec)\n",
    "# plt.scatter(coords2.ra, coords2.dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # warning: slow!!! (not actually that slow)\n",
    "# output = ap.coordinates.search_around_sky(coords1, coords1, maxang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for o in output:\n",
    "#     print((len(o), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(output[2].value, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(output[0], bins=len(np.unique(output[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
