{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifying galaxy environment\n",
    "\n",
    "notebook by _Alex Malz (GCCL@RUB)_, _Kara Ponder (UC Berkeley)_, _Ben Moews (Edinburgh)_, _Stephen Thorp ()_, add your name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import corner\n",
    "import environment as galenv\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "np.seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get the spectra of galaxies matching conditions found [here](http://www.gama-survey.org/dr3/schema/table.php?id=31)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open('SpecObj.fits') as hdul:\n",
    "    hdul.info()\n",
    "#     print(hdul[1].header)\n",
    "    zdf = pd.DataFrame(np.array(hdul[1].data).byteswap().newbyteorder())\n",
    "    print(zdf.columns)\n",
    "#     df.index = df['CATAID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAMA did calculate some environment measures for us, but only on a small subset of galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with fits.open('EnvironmentMeasures.fits') as hdul:\n",
    "#     hdul.info()\n",
    "# #     print(hdul[1].header)\n",
    "#     envdf = pd.DataFrame(np.array(hdul[1].data).byteswap().newbyteorder())\n",
    "#     print(envdf.columns)\n",
    "# #     envdf.index = envdf['CATAID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.merge(envdf, zdf, on='CATAID')\n",
    "df = zdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select spectra by redshift and field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each galaxy in the GAMA catalog has a spectroscopically confirmed redshift.  \n",
    "We're going to match these redshifts to the snapshots of the particle data.\n",
    "\n",
    "Add in a cutoff at lowest redshift at 0.023 and highest at 3.066\n",
    "- Came to this by what the distance is between 0.042 and it's upper endpoint\n",
    "- Added an upper cutoff as well of 3.066 (by same way for equal redshift space on either side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_SLICS = np.array([0.042, 0.080, 0.130, 0.221, 0.317, 0.418, 0.525, 0.640, 0.764, 0.897, \n",
    "           1.041, 1.199, 1.372, 1.562, 1.772, 2.007, 2.269, 2.565, 2.899])\n",
    "z_mids = (z_SLICS[1:] + z_SLICS[:-1]) / 2.\n",
    "z_bins = np.insert(z_mids, 0, 0.023)\n",
    "z_bins = np.append(z_bins, 3.066)\n",
    "print(z_bins)\n",
    "plt.hist(df['Z'], bins=z_bins)\n",
    "plt.semilogy()\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel('number of galaxies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of redshift is skewed by the use of `z=10` as a placeholder for not having a secure redshift.  \n",
    "GAMA has a quality flag we can use to filter for redshifts that were considered of sufficient quality for science use, which they define as `NQ > 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moar_bins = np.arange(z_bins[0], z_bins[-1] + z_bins[1], z_bins[1])\n",
    "for i in range(5):\n",
    "    quality = df.loc[df['NQ'] == i+1, 'Z']\n",
    "    plt.hist(quality, alpha=0.5, label=str(i+1), bins=moar_bins)\n",
    "plt.legend(loc='upper right')\n",
    "plt.semilogy()\n",
    "plt.xlim(moar_bins[0], moar_bins[-1])\n",
    "plt.xlabel('Z')\n",
    "plt.ylabel('number of galaxies')\n",
    "plt.title('redshift distributions by quality flag \"NQ\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAMA observed galaxies in four disjoint regions of the sky.\n",
    "Since environment is about the immediate vicinity of each galaxy, we'll have to divide the galaxies by region, effectively building our redshift-environment-color distribution separately for each region before combining those findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(np.array([df['RA'], df['DEC']]).T, labels=['RA', 'DEC'], show_titles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RA_bin_ends = [0., 80., 160., 200., 360.]\n",
    "subsamples, lens = [], []\n",
    "for j in range(len(z_bins)-1):\n",
    "    for i in range(len(RA_bin_ends)-1):\n",
    "        subsample = df.loc[(df['RA'] >= RA_bin_ends[i]) & (df['RA'] < RA_bin_ends[i+1]) \n",
    "                             & (df['NQ'] > 2) & (df['Z'] >= z_bins[j]) & (df['Z'] < z_bins[j+1]), \n",
    "                             ['CATAID', 'RA', 'DEC', 'Z', 'NQ']]\n",
    "        nn = len(subsample)\n",
    "        if nn > 0:\n",
    "            lens.append(nn)\n",
    "            subsamples.append(subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosen_field = np.argmax(lens)\n",
    "print(chosen_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datum = np.vstack((subsamples[chosen_field]['DEC'], [subsamples[chosen_field]['RA']])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of neighbors within a distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each field, we can quantify the density of the local region around each galaxy, which is really what the notion of \"galaxy environment\" is getting at.\n",
    "We're going to use the number of neighboring galaxies at each of several given distances in angular coordinates, so as not to incur the computational cost of calculating the distances between all galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(galenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose some reasonable radii\n",
    "\n",
    "Our angular positions are in degrees.\n",
    "The distances will be in bogus units because the code normalizes to the radius of the Earth, but we only need the number of neighbors within an angular distance, so it should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosen_ind = np.random.randint(0, len(datum), 1)[0]\n",
    "print(chosen_ind)\n",
    "try_distances = np.flip(np.geomspace(0.01, 1.0, 10), axis=0)\n",
    "res = []\n",
    "friends = datum\n",
    "for dist in try_distances:\n",
    "    friends = galenv.nn_finder(friends, datum[chosen_ind], dist)\n",
    "    res.append(len(friends) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(try_distances, res)\n",
    "plt.xlabel('distance in angular coordinates')\n",
    "plt.ylabel('number of neighbors within distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine distribution of environment values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_env(ind):\n",
    "    res = [subsamples[s]['CATAID'].values[ind]]\n",
    "    friends = data\n",
    "    for dist in try_distances:\n",
    "        friends = galenv.nn_finder(friends, data[ind], dist)\n",
    "        res.append(len(friends))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only execute this cell once, because it's slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do this for all 4 fields and 19 redshifts separately\n",
    "all_envs = []\n",
    "for s in range(len(subsamples)):\n",
    "    print(lens[s])\n",
    "    if lens[s] == 1:\n",
    "        envs_in_field = [[subsamples[s]['CATAID'].values[0]] + [1] * len(try_distances)]\n",
    "    else:\n",
    "        data = np.vstack((subsamples[s]['DEC'], [subsamples[s]['RA']])).T\n",
    "        nps = mp.cpu_count()\n",
    "        pool = mp.Pool(nps - 1)\n",
    "        envs_in_field = pool.map(calc_env, range(len(data)))\n",
    "        pool.close()\n",
    "    all_envs = all_envs + envs_in_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "envs_arr = np.array(all_envs)\n",
    "envs_df = pd.DataFrame(data=envs_arr, index = envs_arr[:, 0], columns = ['CATAID']+[str(i) for i in try_distances])\n",
    "\n",
    "df = pd.merge(envs_df, zdf, on='CATAID')\n",
    "df.to_csv('enviros.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've executed the above once, comment it out and execute the following, to skip the slow step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zenvdf = pd.read_csv('enviros.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zenvdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the number of neighbors for all galaxies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "orig_distances = np.flip(try_distances, axis=0)\n",
    "for i in range(len(orig_distances)):\n",
    "    plt.violinplot(envs_df[str(orig_distances[i])], positions=[i], vert=False)\n",
    "plt.yticks(range(len(orig_distances)), np.around(orig_distances, 3))\n",
    "plt.semilogx()\n",
    "plt.ylabel('distance')\n",
    "plt.xlabel('number of neighbors')\n",
    "#plt.ylim(0.99, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It remains to be seen whether the multimodal distributions in some bins are physical or not.\n",
    "(They look like a problem with smoothing over a discrete variable.)\n",
    "We'll try plotting them as a function of field and redshift next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step:\n",
    "\n",
    "## Constructing the redshift-environment-SED/color relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def redshift_df(str_zbin):\n",
    "    r_files = glob.glob('SpecObjPhot/SpecObjPhot*.csv')\n",
    "    f = [s for s in r_files if str_zbin in s]\n",
    "    phodf = pd.read_csv(f[0])\n",
    "    phodf = phodf.drop(columns=['GAMA_NAME', 'IC_FLAG', \n",
    "                            'N_SPEC', 'N_GAMA_SPEC', 'DIST', \n",
    "                            'SPECID', 'SURVEY', 'SURVEY_CODE',\n",
    "                            'RA', 'DEC', 'WMIN', 'WMAX', 'Z', 'NQ',\n",
    "                            'PROB', 'FILENAME', 'URL', 'URL_IMG'])\n",
    "    df = pd.merge(phodf, zenvdf, on=['CATAID'])\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_0042 = redshift_df('0.042')\n",
    "df = df_0042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### whichdata = ['Z']\n",
    "corner.corner(np.array([df['RA'], df['DEC']]).T, labels=['RA', 'DEC'], show_titles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib\n",
    "\n",
    "jet = plt.cm.Spectral #plt.cm.get_cmap('viridis_r')\n",
    "cNorm  = colors.Normalize(vmin=z_SLICS[0], vmax=z_SLICS[13])\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)\n",
    "color=scalarMap.to_rgba(z_SLICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#z_SLICS\n",
    "try_distances = np.flip(np.geomspace(0.01, 1.0, 10), axis=0)\n",
    "orig_distances = np.flip(try_distances, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "#plt.figure(figsize=(15, 15))\n",
    "for n, z in enumerate(z_SLICS):\n",
    "    df = redshift_df(str(z))\n",
    "    if len(df) > 0:\n",
    "        for i in range(len(orig_distances)):\n",
    "            parts = ax.violinplot(df[str(orig_distances[i])], positions=[i], vert=False)\n",
    "            np.where(df[str(orig_distances[i])] < 1)\n",
    "            c = color[n]\n",
    "            for pc in parts['bodies']:\n",
    "                pc.set_facecolor(c)\n",
    "                pc.set_edgecolor(c)\n",
    "                pc.set_alpha(0.5)\n",
    "            \n",
    "            parts['cbars'].set_color(c)\n",
    "            parts['cbars'].set_alpha(0.7)\n",
    "            \n",
    "            parts['cmaxes'].set_color(c)\n",
    "            parts['cmaxes'].set_alpha(0.7)\n",
    "            \n",
    "            parts['cmins'].set_color(c)\n",
    "            parts['cmins'].set_alpha(0.7)\n",
    "    else:\n",
    "        print(\"I have nothing for you at n=%s, z=%s\"%(n,z))\n",
    "\n",
    "\n",
    "\n",
    "plt.yticks(range(len(orig_distances)), np.around(orig_distances, 3))\n",
    "ax.semilogx()\n",
    "ax.set_ylabel('radial distance [deg]', size=15)\n",
    "ax.set_xlabel('number of neighbors', size=15)\n",
    "\n",
    "cax, _ = matplotlib.colorbar.make_axes(ax, pad=0.01)\n",
    "cbar = matplotlib.colorbar.ColorbarBase(cax, cmap=jet, norm=cNorm)\n",
    "cbar.ax.set_ylabel('redshift', size=12)\n",
    "#plt.ylim(0.99, 10)\n",
    "\n",
    "#plt.savefig('spectral_violinplot_neighbor_v_distance.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = redshift_df(str(z_SLICS[0]))\n",
    "for i in range(len(orig_distances)):\n",
    "    dist = str(orig_distances[i])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sc = plt.scatter(df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lsstg']  \n",
    "                - df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lsstr'],\n",
    "               df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lsstr']  \n",
    "                - df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lssti'],\n",
    "                c=df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)][dist].values,\n",
    "                cmap=plt.cm.Spectral, #'viridis_r',\n",
    "                vmin=1.0,\n",
    "                vmax=max(df['1.0']),\n",
    "                norm=matplotlib.colors.LogNorm(),\n",
    "               alpha=0.4)\n",
    "\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlabel('g-r', size=15)\n",
    "    plt.ylabel('r-i', size=15)\n",
    "    plt.title('dist = %s' %dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_0042 = redshift_df('0.042')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_0042.iterrows():\n",
    "    #print(index)\n",
    "    #if index < 100:\n",
    "    dist = [str(i) for i in orig_distances]\n",
    "    plt.plot(orig_distances, row[dist], 'ok', alpha=0.5)\n",
    "plt.xlabel('radial distance')\n",
    "plt.ylabel('# neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0042 = redshift_df('0.080')\n",
    "df_0042.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_0042.loc[(df_0042['lsstr'] > 0) & (df_0042['lssti'] > 0) & (df_0042['lsstz'] > 0), \n",
    "                [#'lsstg', 'lsstg_err', \n",
    "                 #'lsstr', 'lsstr_err', \n",
    "                 #'lssti', 'lssti_err', \n",
    "                 #'lsstz', 'lsstz_err', \n",
    "                 #'lssty', 'lssty_err',\n",
    "                 '1.0', '0.5994842503189409',\n",
    "                 '0.3593813663804626', '0.21544346900318834', \n",
    "                 '0.1291549665014884', '0.0774263682681127', \n",
    "                 '0.046415888336127774', '0.027825594022071243',\n",
    "                 '0.016681005372000592', '0.01']].values\n",
    "\n",
    "df_0042 = df_0042.loc[(df_0042['lsstr'] > 0) & (df_0042['lssti'] > 0) & (df_0042['lsstz'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km_orig = TimeSeriesKMeans(n_clusters=3, metric=\"euclidean\", max_iter=5, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km_orig.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km_dba = TimeSeriesKMeans(n_clusters=3, metric=\"dtw\", max_iter=5,\n",
    "                          max_iter_barycenter=5,\n",
    "                           random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km_dba.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km_sdtw = TimeSeriesKMeans(n_clusters=3, metric=\"softdtw\", max_iter=5,\n",
    "                            max_iter_barycenter=5,\n",
    "                            metric_params={\"gamma\": .5},\n",
    "                            random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wh_1, = np.where(km_orig.labels_ == 0)\n",
    "wh_2, = np.where(km_orig.labels_ == 1)\n",
    "wh_3, = np.where(km_orig.labels_ == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(df_0042.iloc[wh_1]['lsstr']-df_0042.iloc[wh_1]['lssti'],\n",
    "         df_0042.iloc[wh_1]['lssti']-df_0042.iloc[wh_1]['lsstz'], \n",
    "         'o', alpha=0.4, label='1st Cluster')\n",
    "\n",
    "plt.plot(df_0042.iloc[wh_2]['lsstr']-df_0042.iloc[wh_2]['lssti'],\n",
    "         df_0042.iloc[wh_2]['lssti']-df_0042.iloc[wh_2]['lsstz'],\n",
    "         'o', alpha=0.2, label='2nd Cluster')\n",
    "\n",
    "plt.plot(df_0042.iloc[wh_3]['lsstr']-df_0042.iloc[wh_3]['lssti'],\n",
    "         df_0042.iloc[wh_3]['lssti']-df_0042.iloc[wh_3]['lsstz'],\n",
    "         'o', alpha=0.1, label='3rd Cluster')\n",
    "\n",
    "#plt.xlim(-2, 6)\n",
    "#plt.ylim(-2, 3)\n",
    "\n",
    "plt.xlabel('r-i')\n",
    "plt.ylabel('i-z')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Increase number of clusters \n",
    "# TBD: intelligently pick a number of cluxters\n",
    "km = TimeSeriesKMeans(n_clusters=10, metric=\"euclidean\", max_iter=5, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump model to pickle so we can call it later\n",
    "#redshift dependent\n",
    "os.makedirs('ts_kmeans', exist_ok=True)\n",
    "filename = 'ts_kmeans/tskmeans_0.080.pkl'\n",
    "pickle.dump(km, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_0042['label'] = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_0042['r-i'] = df_0042['lsstr']-df_0042['lssti']\n",
    "df_0042['i-z'] = df_0042['lssti']-df_0042['lsstz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create only the input data columns for Slack\n",
    "#new = df_0042[['r-i', 'i-z', 'lsstr', 'label']]\n",
    "#new.to_csv('colors_lsstr_label_0080.csv')\n",
    "#new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for f in range(0, 10):\n",
    "    #print(f)\n",
    "    wh_1, = np.where(km.labels_ == f)\n",
    "    print(len(wh_1))\n",
    "    plt.scatter(df_0042.iloc[wh_1]['lsstr']-df_0042.iloc[wh_1]['lssti'],\n",
    "                df_0042.iloc[wh_1]['lssti']-df_0042.iloc[wh_1]['lsstz'], \n",
    "                alpha=0.2, #label='%s Cluster' %f,\n",
    "                c=[f]*len(wh_1),  #c=df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)][dist].values\n",
    "                cmap=plt.cm.Spectral, #'viridis_r',\n",
    "                vmin=0,\n",
    "                vmax=10)\n",
    "                #norm=matplotlib.colors.LogNorm())\n",
    "\n",
    "plt.xlim(-2, 3)\n",
    "plt.ylim(-2, 2)\n",
    "\n",
    "plt.ylabel('i-z')\n",
    "plt.xlabel('r-i')\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df_0042\n",
    "fig = corner.corner(np.array([df.loc[(km.labels_ == 0)]['lsstr'], \n",
    "                        df.loc[(km.labels_ == 0)]['lssti'],\n",
    "                        df.loc[(km.labels_ == 0)]['lsstz'],\n",
    "                        #df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lsstg_err'], \n",
    "                        #df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lsstr_err'],\n",
    "                        #df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lssti_err']\n",
    "                       ]).T, \n",
    "              labels=['r', 'i', 'z'], show_titles=True, color='b', plot_density=False,\n",
    "              plot_contours=False,\n",
    "              quantiles=[0.5])\n",
    "\n",
    "corner.corner(np.array([df.loc[(km.labels_ == 4)]['lsstr'], \n",
    "                        df.loc[(km.labels_ == 4)]['lssti'],\n",
    "                        df.loc[(km.labels_ == 4)]['lsstz'],\n",
    "                        #df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lsstg_err'], \n",
    "                        #df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lsstr_err'],\n",
    "                        #df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lssti_err']\n",
    "                       ]).T, \n",
    "              labels=['r', 'i', 'z'], show_titles=True, color='r', fig=fig, plot_density=False, \n",
    "              plot_contours=False,\n",
    "              quantiles=[0.5])\n",
    "\n",
    "corner.corner(np.array([df.loc[(km.labels_ == 9)]['lsstr'], \n",
    "                        df.loc[(km.labels_ == 9)]['lssti'],\n",
    "                        df.loc[(km.labels_ == 9)]['lsstz'],\n",
    "                        #df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lsstg_err'], \n",
    "                        #df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lsstr_err'],\n",
    "                        #df.loc[(df['lsstg'] > 0) & (df['lsstr'] > 0) & (df['lssti'] > 0)]['lssti_err']\n",
    "                       ]).T, \n",
    "              labels=['r', 'i', 'z'], show_titles=True, color='m', fig=fig, plot_density=False, \n",
    "              plot_contours=False,\n",
    "              quantiles=[0.16, 0.84])\n",
    "\n",
    "\n",
    "# Extract the axes\n",
    "#axes = np.array(figure.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Loop over the diagonal\n",
    "#for i in range(ndim):\n",
    "#    ax = axes[i, i]\n",
    "#    ax.axvline(value1[i], color=\"g\")\n",
    "#    ax.axvline(value2[i], color=\"r\")\n",
    "#fig.savefig('tsclustering.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate 3D Multivariate normal \n",
    "\n",
    "Notes:\n",
    "tavy_resampled is drawing `N_resamples` number of samples from our distribution. If the standard deviation on the mu's and Sigma's are low, then stats.multivariate_normal is fine. If there are large errors, then the tavy_resampled take these errors into account when drawing. \n",
    "\n",
    "mu_trans, sigma, L_Omega, L_Sigma, Omega are by-products of the fits. There can be nan's in L_Omega and L_Sigma as they are triangle matrices.\n",
    "\n",
    "mu[1, 2, 3] = 3 means\n",
    "Sigma[] = covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pystan\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('colors_lsstr_label_0080.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_l = df.loc[df['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = len(df_l)\n",
    "\n",
    "vals = df_l[[ 'r-i', 'i-z', 'lsstr']].values\n",
    "\n",
    "N_re = 1\n",
    "\n",
    "dat = {\n",
    "    'N': N,\n",
    "    'D': 3,\n",
    "    'N_resamples': N_re,\n",
    "    'tavy': vals,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You only need this ONE model to fix all of the data no matter what label or redshift you have!\n",
    "try:\n",
    "    sm = pickle.load(open('3D_Gaussian_model.pkl', 'rb'))\n",
    "except:\n",
    "    sm = pystan.StanModel(file='some_dimensional_gaussian.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model so that you don't have to keep regenerating it. \n",
    "#with open('3D_Gaussian_model.pkl', 'wb') as f:\n",
    "#    pickle.dump(sm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sm.sampling(data=dat, iter=1000, chains=4, warmup=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit.stansummary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(fit.summary(pars=['mu', 'Sigma'])['summary'], \n",
    "             columns=fit.summary(pars=['mu', 'Sigma'])['summary_colnames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['Names'] = fit.summary(pars=['mu', 'Sigma'])['summary_rownames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('fit_summaries', exist_ok=True)\n",
    "summary.to_csv('fit_summaries/summary_0.080_label_9.csv') ## \n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = summary.iloc[:3]['mean'].values\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df_l['r-i'], bins=np.arange(-0.5, 1.5, 0.15), alpha=0.5, label='r-i')\n",
    "plt.hist(df_l['i-z'], bins=np.arange(-0.5, 1.5, 0.15), alpha=0.5, label='i-z')\n",
    "#plt.hist(df_l['i-z'])\n",
    "\n",
    "plt.axvline(mus[0], color='k')\n",
    "plt.axvline(mus[1], color='k')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df_l['lsstr'], label='lsstr')\n",
    "\n",
    "plt.axvline(mus[2], color='k')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the means are so well defined, we will use scipy to draw random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = summary.iloc[:3]['mean'].values\n",
    "cov = summary.iloc[3:]['mean'].values.reshape(3, 3)\n",
    "\n",
    "rando = multivariate_normal.rvs(mus, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(df_l['r-i'], bins=np.arange(-0.5, 1.5, 0.15), alpha=0.5, label='r-i')\n",
    "plt.hist(df_l['i-z'], bins=np.arange(-0.5, 1.5, 0.15), alpha=0.5, label='i-z')\n",
    "#plt.hist(df_l['i-z'])\n",
    "\n",
    "plt.axvline(mus[0], color='k', label='fit means')\n",
    "plt.axvline(mus[1], color='k')\n",
    "\n",
    "plt.axvline(rando[0], color='grey', lw=5, alpha=0.5, label='Random Draw')\n",
    "plt.axvline(rando[1], color='grey', lw=5, alpha=0.5)\n",
    "plt.xlabel('color [mag]')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df_l['lsstr'], label='lsstr')\n",
    "\n",
    "plt.axvline(mus[2], color='k', label='fit means')\n",
    "\n",
    "plt.axvline(rando[2], color='grey', alpha=0.5, lw=5, label='Random Draw')\n",
    "plt.xlabel('lsstr [mag]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
